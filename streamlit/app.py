from pyspark.sql import SparkSession
from pyspark.sql.functions import countDistinct, when, col, explode, split, corr, expr, rand, percentile_approx, avg
import plotly.express as px
import pandas as pd
import streamlit as st
from streamlit_option_menu import option_menu
import plotly.graph_objects as go
import numpy as np
import time

# Configura√ß√£o da p√°gina Streamlit
st.set_page_config(
    page_title="An√°lise Explorat√≥ria - Recomendador de Not√≠cias",
    page_icon="üìä",
    layout="wide"
)

# Inicializa√ß√£o do Spark
@st.cache_resource
def init_spark():
    spark = SparkSession.builder.appName("NewsRecommenderEDA") \
            .config("spark.sql.parquet.datetimeRebaseModeInRead", "CORRECTED") \
            .config("spark.sql.parquet.datetimeRebaseModeInWrite", "CORRECTED") \
            .config("spark.driver.memory", "8g") \
            .config("spark.executor.memory", "8g") \
            .config("spark.sql.shuffle.partitions", "10") \
            .config("spark.sql.autoBroadcastJoinThreshold", "10m") \
            .config("spark.memory.fraction", "0.8") \
            .config("spark.memory.storageFraction", "0.3") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .config("spark.sql.inMemoryColumnarStorage.compressed", "true") \
            .config("spark.sql.inMemoryColumnarStorage.batchSize", "10000") \
            .config("spark.sql.shuffle.partitions", "8") \
            .getOrCreate()
    return spark


def load_data(spark):
    treino = spark.read \
        .option("mergeSchema", "false") \
        .parquet("datalake/silver/treino") \
        .coalesce(2)
    
    itens = spark.read \
        .option("mergeSchema", "false") \
        .parquet("datalake/silver/itens") \
        .coalesce(2)
    
    return treino, itens

@st.cache_data
def get_news_per_user(_spark):
    """Cache para os dados da An√°lise 1"""
    return _spark.sql("""
        SELECT userId, COUNT(history) as news_count
        FROM tab_treino
        GROUP BY userId
    """).toPandas()

@st.cache_data
def get_time_distribution(_spark):
    """Cache para os dados da An√°lise 2"""
    time_dist = _spark.sql("""
        SELECT hour, COUNT(*) as count
        FROM tab_treino
        GROUP BY hour
        ORDER BY hour
    """).toPandas()
    
    time_week_dist = _spark.sql("""
        SELECT dayofweek, hour, COUNT(*) as count
        FROM tab_treino
        GROUP BY dayofweek, hour
        ORDER BY dayofweek, hour
    """).toPandas()
    
    time_series = _spark.sql("""
        SELECT year, month, day, COUNT(*) as count
        FROM tab_treino
        GROUP BY year, month, day
        ORDER BY year, month, day
    """).toPandas()
    
    # Criar a coluna year_month_day
    time_series['year_month_day'] = time_series['year'].astype(str) + '-' + \
                                   time_series['month'].astype(str) + '-' + \
                                   time_series['day'].astype(str)
    
    return time_dist, time_week_dist, time_series

@st.cache_data
def get_engagement_data(_spark):
    """Cache para os dados da An√°lise 3"""
    engagement_df = _spark.sql("""
        SELECT history,
            timeOnPageHistory, 
            numberOfClicksHistory, 
            scrollPercentageHistory, 
            interaction_score
        FROM tab_treino
    """)

    engagement_df_grouped = engagement_df.groupBy("history").agg(
        avg("timeOnPageHistory").alias("avg_timeOnPageHistory"),
        avg("numberOfClicksHistory").alias("avg_numberOfClicksHistory"),
        avg("scrollPercentageHistory").alias("avg_scrollPercentageHistory"),
        avg("interaction_score").alias("avg_interaction_score")
    )

    filtered_df_grouped = engagement_df_grouped.filter(
        (col("avg_timeOnPageHistory") > 0) & 
        (col("avg_timeOnPageHistory") < 500000) & 
        (col("avg_numberOfClicksHistory") < 500)  
    )

    return filtered_df_grouped.sample(fraction=0.1).toPandas()

@st.cache_data
def get_retention_data(_spark):
    """Cache para os dados da An√°lise 4"""
    retention_df = _spark.sql("""
        SELECT year, month, day, userId, COUNT(*) as visits
        FROM tab_treino
        GROUP BY year, month, day, userId
    """).toPandas()

    time_retention_df = _spark.sql("""
        SELECT year, month, day, COUNT(DISTINCT userId) as unique_users
        FROM tab_treino
        GROUP BY year, month, day
        ORDER BY year, month, day
    """).toPandas()

    # Criar a coluna date
    time_retention_df["date"] = pd.to_datetime(
        time_retention_df[["year", "month", "day"]].assign(
            year=lambda x: x["year"].astype(str),
            month=lambda x: x["month"].astype(str).str.zfill(2),
            day=lambda x: x["day"].astype(str).str.zfill(2)
        ).agg("-".join, axis=1)
    )

    return retention_df, time_retention_df

@st.cache_data
def get_user_type_data(_spark):
    """Cache para os dados da An√°lise 5"""
    return _spark.sql("""
        SELECT 
            CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS date, 
            userType, 
            COUNT(*) AS count
        FROM tab_treino
        GROUP BY year, month, day, userType
        ORDER BY date
    """).toPandas()

@st.cache_data
def get_recency_data(_spark):
    """Cache para os dados da An√°lise 6"""
    return _spark.sql("""
        SELECT i.year as item_year, i.month as item_month, i.day as item_day, 
               i.days_since_published, COUNT(*) as access_count
        FROM tab_treino AS t
        JOIN tab_itens AS i
        ON t.history = i.page
        GROUP BY i.year, i.month, i.day, i.days_since_published
    """).toPandas()

@st.cache_data
def get_news_overlap_data(_spark):
    """Cache para os dados da An√°lise 7"""
    return _spark.sql("""
        SELECT t.history AS news_id, COUNT(DISTINCT t.userId) AS user_count
        FROM tab_treino AS t
        GROUP BY t.history
    """).toPandas()

@st.cache_data
def get_news_popularity_data(_spark):
    """Cache para os dados de popularidade das not√≠cias (An√°lise 7 e 8)"""
    # Query para obter array de not√≠cias
    news_popularity_df = _spark.sql("""
        SELECT userId, split(history, ',') AS news_array
        FROM tab_treino
    """).withColumn("news_id", explode(col("news_array")))

    # Agrega√ß√£o por not√≠cia
    news_popularity_df = news_popularity_df.groupBy("news_id") \
        .agg(countDistinct("userId").alias("unique_users"))

    # Converter para pandas para processamento posterior
    return news_popularity_df.toPandas()

@st.cache_data
def get_clicks_time_correlation(_spark):
    """Cache para os dados da An√°lise 9"""
    correlation = _spark.sql("""
        SELECT corr(numberOfClicksHistory, timeOnPageHistory) as correlation 
        FROM tab_treino
    """).collect()[0]["correlation"]
    
    sample_df = _spark.sql("""
        SELECT numberOfClicksHistory, timeOnPageHistory 
        FROM tab_treino
    """).sample(fraction=0.1, seed=42).toPandas()
    
    return correlation, sample_df

@st.cache_data
def get_user_interactions_data(_spark):
    """Cache para os dados da An√°lise 10"""
    return _spark.sql("""
        SELECT userId, COUNT(history) AS interaction_count
        FROM tab_treino
        GROUP BY userId
    """).toPandas()

def show_analysis_1(spark):  
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 1: Distribui√ß√£o do n√∫mero de not√≠cias lidas por usu√°rio</h1>", unsafe_allow_html=True)
    
    st.markdown("""
    üéØ **Objetivo**
    - Compreender o padr√£o de consumo de not√≠cias pelos usu√°rios
    - Identificar se a distribui√ß√£o do consumo √© equilibrada ou se h√° concentra√ß√£o em poucos usu√°rios
    - Detectar poss√≠veis outliers, como usu√°rios altamente engajados ou bots
    - Ajudar a definir estrat√©gias diferenciadas de recomenda√ß√£o para usu√°rios casuais e frequentes
    ------------------------------------------------------------
    """)

    news_per_user = get_news_per_user(spark)

    fig_hist = px.histogram(news_per_user, x='news_count', nbins=30, 
                          title='Distribui√ß√£o de Not√≠cias Lidas por Usu√°rio')
    st.plotly_chart(fig_hist, use_container_width=True)

    st.markdown("""
    üìä **Gr√°fico 1: Histograma da Distribui√ß√£o de Not√≠cias Lidas por Usu√°rio**

    üîé **Observa√ß√µes:**
    - A grande maioria dos usu√°rios l√™ poucas not√≠cias, o que indica que o consumo √© altamente concentrado
    - H√° uma cauda longa na distribui√ß√£o, com alguns usu√°rios lendo milhares de not√≠cias
    - Isso pode indicar a presen√ßa de usu√°rios extremamente engajados ou at√© mesmo bots

    üßê **O que isso significa para o modelo?**
    - A recomenda√ß√£o personalizada pode ser mais relevante para usu√°rios que consomem muitas not√≠cias, pois h√° mais dados sobre suas prefer√™ncias
    - Para usu√°rios casuais, recomenda√ß√µes baseadas em popularidade ou tend√™ncias podem ser mais eficazes

    ‚úÖ **A√ß√£o recomendada:**
    - Separar os usu√°rios em grupos (casuais, medianos e altamente engajados) para testar recomenda√ß√µes diferenciadas
    - Filtrar poss√≠veis bots ao identificar usu√°rios com consumo anormalmente alto
    - Criar estrat√©gias para engajar usu√°rios com poucos acessos, oferecendo recomenda√ß√µes mais diversificadas ou guiadas por tend√™ncias
    ------------------------------------------------------------
    """)

    fig_box = px.box(news_per_user, y='news_count', 
                     title='Distribui√ß√£o de Not√≠cias Lidas por Usu√°rio (Boxplot)')
    st.plotly_chart(fig_box, use_container_width=True)

    st.markdown("""
    üìä **Gr√°fico 2: Boxplot da Distribui√ß√£o de Not√≠cias Lidas por Usu√°rio**

    üîé **Observa√ß√µes:**
    - O boxplot evidencia a presen√ßa de outliers extremos, que se destacam do restante da distribui√ß√£o
    - A grande maioria dos usu√°rios consome um n√∫mero pequeno de not√≠cias, enquanto alguns poucos consomem milhares

    üßê **O que isso significa para o modelo?**
    - Como os outliers podem distorcer m√©tricas m√©dias e padr√µes de recomenda√ß√£o, √© importante trat√°-los adequadamente
    - O modelo pode precisar de pesos diferentes para usu√°rios casuais e altamente engajados

    ‚úÖ **A√ß√£o recomendada:**
    - Remover outliers extremos ou trat√°-los separadamente para evitar distor√ß√µes
    - Criar um modelo h√≠brido, onde a recomenda√ß√£o para usu√°rios frequentes seja altamente personalizada e a recomenda√ß√£o para novos usu√°rios seja baseada em popularidade
    - Considerar limites superiores para o n√∫mero de not√≠cias lidas ao calcular estat√≠sticas m√©dias
    ------------------------------------------------------------
    """)

    st.markdown(
        """
    üìå **Conclus√µes Finais**

    üöÄ **Impacto no modelo de recomenda√ß√£o:**

    - O comportamento de consumo √© muito desigual, exigindo abordagens diferentes para diferentes perfis de usu√°rios.
    - Modelos baseados em popularidade podem ser √∫teis para novos usu√°rios, enquanto modelos mais personalizados beneficiam os usu√°rios mais engajados.
    - O tratamento de outliers e segmenta√ß√£o de usu√°rios pode melhorar a precis√£o e a relev√¢ncia das recomenda√ß√µes.
    ------------------------------------------------------------
    """)

def show_analysis_2(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 2: Distribui√ß√£o temporal das intera√ß√µes dos usu√°rios</h1>", unsafe_allow_html=True)

    st.markdown(
        """
        üéØ **Objetivo**
        - Identificar hor√°rios de pico de leitura e padr√µes sazonais.
        - Verificar diferen√ßas no comportamento de leitura entre diferentes hor√°rios e dias da semana.
        - Fornecer insights para a introdu√ß√£o de features temporais no modelo de recomenda√ß√£o.
        - Entender a evolu√ß√£o das intera√ß√µes ao longo do tempo para identificar tend√™ncias.
        ------------------------------------------------------------
        """
    )

    time_dist, time_week_dist, time_series = get_time_distribution(spark)

    fig_time = px.bar(time_dist, x='hour', y='count', title='Distribui√ß√£o de Acessos por Hora do Dia')
    st.plotly_chart(fig_time, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 1: Distribui√ß√£o de Acessos por Hora do Dia**

        üîé **Observa√ß√µes:**
        - O volume de acessos √© menor na madrugada e atinge picos a partir das 10h, com um crescimento cont√≠nuo at√© o in√≠cio da noite.
        - O maior volume de acessos ocorre entre 12h e 18h, indicando que este pode ser um per√≠odo cr√≠tico para recomenda√ß√µes personalizadas.
        - O per√≠odo da madrugada apresenta acessos mais baixos, sugerindo que a atividade dos usu√°rios √© m√≠nima entre 2h e 6h.

        üßê **O que isso significa para o modelo?**
        - Modelos baseados em rec√™ncia podem precisar considerar a hora do dia para evitar recomendar conte√∫dos fora do hor√°rio de maior engajamento.
        - Recomenda√ß√µes feitas pela manh√£ podem se beneficiar de tend√™ncias do dia anterior, enquanto √† noite podem ser baseadas no consumo do pr√≥prio dia.

        ‚úÖ **A√ß√£o recomendada:**
        - Criar um fator de ajuste temporal para favorecer recomenda√ß√µes em hor√°rios de pico.
        - Testar modelos de recomenda√ß√£o que diferenciam usu√°rios matutinos e noturnos.
        - Analisar se a taxa de convers√£o das recomenda√ß√µes varia ao longo do dia.
        ------------------------------------------------------------
        """
    )

    fig_heatmap = px.density_heatmap(time_week_dist, x='hour', y='dayofweek', z='count', title='Heatmap de Acessos por Hora e Dia da Semana', color_continuous_scale='Blues')
    st.plotly_chart(fig_heatmap, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 2: Heatmap de Acessos por Hora e Dia da Semana**

        üîé **Observa√ß√µes:**
        - Os acessos aumentam durante o hor√°rio comercial e in√≠cio da noite, com picos mais evidentes nos dias √∫teis.
        - O fim de semana apresenta um padr√£o de acessos mais distribu√≠do, sem picos t√£o intensos quanto os dias √∫teis.
        - O per√≠odo da manh√£ durante os dias √∫teis tem um volume crescente de acessos, enquanto nos finais de semana esse crescimento √© menos acentuado.

        üßê **O que isso significa para o modelo?**
        - Usu√°rios podem ter padr√µes de leitura distintos entre dias √∫teis e finais de semana.
        - Recomenda√ß√µes podem ser otimizadas levando em conta a sazonalidade do dia da semana.
        - Not√≠cias mais acessadas durante a semana podem perder relev√¢ncia no final de semana, sugerindo que a rec√™ncia pode ter impacto diferenciado.

        ‚úÖ **A√ß√£o recomendada:**
        - Criar features temporais no modelo de recomenda√ß√£o considerando o dia da semana e hor√°rio.
        - Testar se recomenda√ß√µes de tend√™ncias da semana funcionam no final de semana ou se precisam ser ajustadas.
        - Analisar se o engajamento do usu√°rio varia conforme o dia e ajustar a estrat√©gia de recomenda√ß√£o.
        ------------------------------------------------------------
        """
    )

    fig_line = px.line(time_series, x='year_month_day', y='count', title='Evolu√ß√£o das Intera√ß√µes ao Longo do Tempo')
    st.plotly_chart(fig_line, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 3: Evolu√ß√£o das Intera√ß√µes ao Longo do Tempo**

        üîé **Observa√ß√µes:**
        - H√° flutua√ß√µes regulares no volume de intera√ß√µes, possivelmente refletindo um ciclo semanal de consumo de not√≠cias.
        - Os acessos tendem a diminuir em alguns per√≠odos espec√≠ficos, sugerindo sazonalidade.
        - Alguns picos de intera√ß√£o podem estar associados a eventos de grande impacto.

        üßê **O que isso significa para o modelo?**
        - A rec√™ncia e a sazonalidade s√£o fatores cr√≠ticos para a recomenda√ß√£o de not√≠cias.
        - Eventos sazonais podem influenciar fortemente o consumo de not√≠cias, e o modelo deve ser capaz de se adaptar rapidamente.
        - Recomenda√ß√µes baseadas em tend√™ncias podem precisar de ajustes dependendo do dia da semana e per√≠odo do m√™s.

        ‚úÖ **A√ß√£o recomendada:**
        - Criar um mecanismo de ajuste din√¢mico para recomenda√ß√µes baseadas na varia√ß√£o da demanda ao longo do tempo.
        - Explorar a inclus√£o de eventos sazonais no modelo de recomenda√ß√£o.
        - Monitorar a taxa de aceita√ß√£o das recomenda√ß√µes ao longo do tempo para identificar padr√µes e poss√≠veis melhorias.
        ------------------------------------------------------------
        """
    )

    st.markdown(
        """
        üìå **Conclus√µes Finais**

        üöÄ **Impacto no modelo de recomenda√ß√£o:**

        - Hor√°rios e dias da semana influenciam fortemente o consumo de not√≠cias, o que pode ser explorado no modelo.
        - A rec√™ncia deve ser considerada em diferentes escalas temporais para manter a relev√¢ncia das recomenda√ß√µes.
        - Modelos temporais podem melhorar a precis√£o das sugest√µes ao considerar padr√µes de engajamento di√°rios e semanais.
        - A adapta√ß√£o a eventos sazonais pode otimizar a experi√™ncia do usu√°rio, garantindo que as recomenda√ß√µes permane√ßam relevantes.
        ------------------------------------------------------------
        """
    )

def show_analysis_3(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 3: Rela√ß√£o entre tempo na p√°gina e engajamento</h1>", unsafe_allow_html=True)
    
    st.markdown("""
    üéØ **Objetivo**
    - Avaliar se o tempo que um usu√°rio passa em uma p√°gina est√° relacionado com seu n√≠vel de engajamento.
    - Identificar padr√µes entre tempo na p√°gina e diferentes m√©tricas de intera√ß√£o, como n√∫mero de cliques, porcentagem de scroll e score de intera√ß√£o.
    - Compreender se o tempo na p√°gina pode ser um indicador relevante para o modelo de recomenda√ß√£o.
    - Determinar se existem outliers ou comportamentos an√¥malos que devem ser tratados.
    ------------------------------------------------------------
    """)

    sample_df_grouped = get_engagement_data(spark)
    sample_df_grouped['avg_timeOnPageHistory'] = sample_df_grouped['avg_timeOnPageHistory'] / 60000

    fig_scatter_clicks = px.scatter(
        sample_df_grouped, x='avg_timeOnPageHistory', y='avg_numberOfClicksHistory',
        title='Rela√ß√£o entre Tempo na P√°gina e N√∫mero de Cliques',
        labels={'avg_timeOnPageHistory': 'M√©dia de Tempo na P√°gina (minutos)', 
                'avg_numberOfClicksHistory': 'M√©dia de N√∫mero de Cliques'},
        trendline='ols',
        trendline_color_override='red' 
    )
    st.plotly_chart(fig_scatter_clicks, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 1: Rela√ß√£o entre Tempo na P√°gina e N√∫mero de Cliques**

        üîé **Observa√ß√µes:**
        - Existe uma leve tend√™ncia de aumento no n√∫mero de cliques conforme o tempo na p√°gina cresce.
        - No entanto, a dispers√£o dos dados √© alta, o que sugere que o tempo na p√°gina, isoladamente, n√£o determina o n√∫mero de cliques.
        - H√° alguns outliers com muitos cliques, possivelmente indicando p√°ginas com conte√∫do altamente interativo.

        üßê **O que isso significa para o modelo?**
        - O n√∫mero de cliques pode ser um fator de engajamento relevante, mas n√£o √© um indicador determin√≠stico.
        - Pode ser necess√°rio combinar essa m√©trica com outras para criar uma feature mais robusta.

        ‚úÖ **A√ß√£o recomendada:**
        - Criar uma feature combinada entre tempo na p√°gina e cliques, normalizando os valores.
        - Investigar se h√° um limite de tempo al√©m do qual os cliques n√£o aumentam significativamente.
        ------------------------------------------------------------
        """)

    fig_scatter_scroll = px.scatter(
        sample_df_grouped, x='avg_timeOnPageHistory', y='avg_scrollPercentageHistory',
        title='Rela√ß√£o entre Tempo na P√°gina e Scroll (%)',
        labels={'avg_timeOnPageHistory': 'M√©dia de Tempo na P√°gina (minutos)', 
                'avg_scrollPercentageHistory': 'M√©dia de Porcentagem de Scroll'},
        trendline='ols',
        trendline_color_override='red' 
    )
    st.plotly_chart(fig_scatter_scroll, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 2: Rela√ß√£o entre Tempo na P√°gina e Porcentagem de Scroll**

        üîé **Observa√ß√µes:**
        - A maioria dos usu√°rios parece realizar pouco scroll, independentemente do tempo que passam na p√°gina.
        - Alguns outliers indicam sess√µes com scroll muito alto, possivelmente erros de registro ou comportamentos espec√≠ficos.

        üßê **O que isso significa para o modelo?**
        - O scroll pode n√£o ser um indicador confi√°vel de engajamento, especialmente quando o tempo na p√°gina √© longo, mas o scroll √© baixo.
        - Pode haver casos em que os usu√°rios deixam a p√°gina aberta sem interagir com ela.

        ‚úÖ **A√ß√£o recomendada:**
        - Analisar se h√° um threshold m√≠nimo de scroll para considerar a intera√ß√£o v√°lida.
        - Combinar o scroll com outras m√©tricas, como tempo m√©dio de leitura e cliques.
        ------------------------------------------------------------
        """)

    fig_scatter_interaction = px.scatter(
        sample_df_grouped, x='avg_timeOnPageHistory', y='avg_interaction_score',
        title='Rela√ß√£o entre Tempo na P√°gina e Score de Intera√ß√£o',
        labels={'avg_timeOnPageHistory': 'M√©dia de Tempo na P√°gina (minutos)', 
                'avg_interaction_score': 'M√©dia de Score de Intera√ß√£o'},
        trendline='ols',
        trendline_color_override='red' 
    )
    st.plotly_chart(fig_scatter_interaction, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 3: Rela√ß√£o entre Tempo na P√°gina e Score de Intera√ß√£o**

        üîé **Observa√ß√µes:**
        - A rela√ß√£o entre tempo na p√°gina e o score de intera√ß√£o √© quase linear, sugerindo que quanto mais tempo um usu√°rio passa na p√°gina, maior √© seu score de intera√ß√£o.
        - Isso indica que o score de intera√ß√£o j√° pode estar incorporando o tempo de leitura como um fator relevante.

        üßê **O que isso significa para o modelo?**
        - O score de intera√ß√£o parece ser um bom indicador de envolvimento do usu√°rio com a not√≠cia.
        - Esse score pode ser mais √∫til do que usar tempo na p√°gina ou cliques isoladamente.

        ‚úÖ **A√ß√£o recomendada:**
        - Utilizar o score de intera√ß√£o como uma feature central no modelo de recomenda√ß√£o.
        - Testar o impacto de combinar essa m√©trica com cliques e tempo de leitura para melhorar a precis√£o das recomenda√ß√µes.
        ------------------------------------------------------------
        """)

    st.markdown(
        """
        üìå **Conclus√µes Finais**

        üöÄ **Impacto no modelo de recomenda√ß√£o:**

        - O tempo na p√°gina tem correla√ß√£o com o score de intera√ß√£o, mas n√£o necessariamente com o n√∫mero de cliques ou a porcentagem de scroll.
        - A porcentagem de scroll pode n√£o ser um bom indicador isolado e pode precisar de ajustes no modelo.
        - Not√≠cias com altos scores de intera√ß√£o devem ser priorizadas, pois refletem um engajamento mais realista.
        - O modelo pode se beneficiar de uma feature combinada que englobe tempo na p√°gina, cliques e score de intera√ß√£o.
        ------------------------------------------------------------
        """)

def show_analysis_4(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 4: Taxa de retorno dos usu√°rios ao longo do tempo</h1>", unsafe_allow_html=True)

    st.markdown(
        """
        üéØ **Objetivo**
        - Identificar a frequ√™ncia com que os usu√°rios retornam √† plataforma.
        - Analisar padr√µes de retorno ao longo do tempo para entender tend√™ncias e sazonalidade.
        - Avaliar se h√° um padr√£o de fideliza√ß√£o dos usu√°rios, diferenciando usu√°rios casuais e recorrentes.
        - Fornecer insights para estrat√©gias de reten√ß√£o e engajamento no modelo de recomenda√ß√£o.
        ------------------------------------------------------------
        """
    )

    retention_df, time_retention_df = get_retention_data(spark)

    fig_hist_retention = px.histogram(
        retention_df, x="visits", nbins=30,
        title="Distribui√ß√£o da Taxa de Retorno dos Usu√°rios",
        labels={"visits": "N√∫mero de Retornos"}
    )
    st.plotly_chart(fig_hist_retention, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 1: Distribui√ß√£o da Taxa de Retorno dos Usu√°rios**

        üîé **Observa√ß√µes:**
        - A maioria dos usu√°rios acessa a plataforma poucas vezes, com uma distribui√ß√£o altamente concentrada nos primeiros retornos.
        - H√° uma cauda longa indicando que um pequeno n√∫mero de usu√°rios retorna v√°rias vezes, chegando a mais de 40 retornos.
        - Essa discrep√¢ncia sugere a presen√ßa de dois perfis distintos: usu√°rios casuais e usu√°rios altamente engajados.

        üßê **O que isso significa para o modelo?**
        - O modelo de recomenda√ß√£o pode beneficiar-se da diferencia√ß√£o entre usu√°rios casuais e recorrentes.
        - Usu√°rios casuais podem receber recomenda√ß√µes baseadas em popularidade e tend√™ncias gerais.
        - Usu√°rios recorrentes podem receber recomenda√ß√µes mais personalizadas, baseadas em hist√≥rico detalhado de navega√ß√£o.

        ‚úÖ **A√ß√£o recomendada:**
        - Criar segmenta√ß√µes de usu√°rios com base no n√∫mero de retornos para oferecer experi√™ncias diferenciadas.
        - Implementar estrat√©gias para engajar usu√°rios com poucos retornos e incentivar novas visitas.
        - Avaliar se padr√µes de retorno se correlacionam com outros fatores como tempo na p√°gina e n√∫mero de cliques.
        ------------------------------------------------------------
        """
    )

    fig_line_retention = px.line(
        time_retention_df, x="date", y="unique_users",
        title="Evolu√ß√£o da Taxa de Retorno dos Usu√°rios ao Longo do Tempo",
        labels={"date": "Data", "unique_users": "Usu√°rios √önicos por Dia"}
    )
    st.plotly_chart(fig_line_retention, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 2: Evolu√ß√£o da Taxa de Retorno dos Usu√°rios ao Longo do Tempo**

        üîé **Observa√ß√µes:**
        - A taxa de retorno dos usu√°rios segue um padr√£o c√≠clico, com quedas peri√≥dicas seguidas de aumentos bruscos.
        - Esses picos podem estar relacionados a eventos espec√≠ficos, como not√≠cias de grande repercuss√£o que atraem mais usu√°rios.
        - A tend√™ncia geral mostra uma varia√ß√£o significativa ao longo do tempo, indicando que o engajamento n√£o √© constante.

        üßê **O que isso significa para o modelo?**
        - A sazonalidade pode impactar a efici√™ncia do modelo de recomenda√ß√£o, pois a base de usu√°rios ativos varia ao longo do tempo.
        - O modelo pode se beneficiar de features temporais para ajustar recomenda√ß√µes com base no momento da intera√ß√£o.
        - Estrat√©gias de reten√ß√£o podem ser refor√ßadas em per√≠odos de baixa intera√ß√£o para evitar perda de usu√°rios.

        ‚úÖ **A√ß√£o recomendada:**
        - Analisar eventos e fatores que podem estar influenciando os picos de retorno.
        - Criar um sistema de recomenda√ß√£o din√¢mico que se adapte a padr√µes sazonais de engajamento.
        - Desenvolver estrat√©gias para manter usu√°rios ativos mesmo em per√≠odos de baixa intera√ß√£o.
        ------------------------------------------------------------
        """
    )

    st.markdown(
        """
        üìå **Conclus√µes Finais**

        üöÄ **Impacto no modelo de recomenda√ß√£o:**

        - A reten√ß√£o de usu√°rios √© um fator cr√≠tico para o sucesso da recomenda√ß√£o personalizada.
        - A segmenta√ß√£o entre usu√°rios casuais e recorrentes pode melhorar a assertividade das recomenda√ß√µes.
        - Incorporar vari√°veis temporais e padr√µes sazonais pode tornar o modelo mais robusto e responsivo √†s mudan√ßas no comportamento dos usu√°rios.
        - Estrat√©gias de engajamento devem ser direcionadas para aumentar a taxa de retorno, garantindo uma base de usu√°rios ativa e crescente.
        ------------------------------------------------------------
        """
    )

def show_analysis_5(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 5: Propor√ß√£o de usu√°rios logados vs. an√¥nimos</h1>", unsafe_allow_html=True)

    st.markdown(
        """
        üéØ **Objetivo**
        - Compreender a distribui√ß√£o entre usu√°rios logados e an√¥nimos ao longo do tempo.
        - Verificar se h√° mudan√ßas sazonais na propor√ß√£o de usu√°rios logados e an√¥nimos.
        - Avaliar o impacto desse fator na personaliza√ß√£o das recomenda√ß√µes, visto que usu√°rios logados possuem hist√≥rico mais completo.
        - Adaptar as estrat√©gias de recomenda√ß√£o para cada perfil de usu√°rio.
        ------------------------------------------------------------
        """
    )

    user_type_time_df = get_user_type_data(spark)

    user_type_time_df["date"] = pd.to_datetime(user_type_time_df["date"])

    color_map = {
        "Logged": "#1f77b4", 
        "Non-Logged": "#aec7e8"
    }

    fig_area_user_type = px.area(
        user_type_time_df, x="date", y="count", color="userType",
        title="Evolu√ß√£o da Propor√ß√£o de Usu√°rios Logados vs. An√¥nimos",
        labels={"userType": "Tipo de Usu√°rio", "count": "Quantidade", "date": "Data"},
        color_discrete_map=color_map
    )
    st.plotly_chart(fig_area_user_type, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 1: Evolu√ß√£o da Propor√ß√£o de Usu√°rios Logados vs. An√¥nimos (√Årea)**

        üîé **Observa√ß√µes:**
        - A propor√ß√£o de usu√°rios logados e an√¥nimos se mant√©m relativamente est√°vel ao longo do tempo.
        - Pequenas oscila√ß√µes podem indicar eventos ou fatores externos que afetam o login dos usu√°rios.
        - A maior propor√ß√£o de usu√°rios an√¥nimos pode impactar a qualidade da recomenda√ß√£o personalizada, pois h√° menos dados hist√≥ricos dispon√≠veis para esses usu√°rios.

        üßê **O que isso significa para o modelo de recomenda√ß√£o?**
        - Usu√°rios logados t√™m um hist√≥rico de intera√ß√µes mais rico, permitindo recomenda√ß√µes mais personalizadas e sofisticadas.
        - Para usu√°rios an√¥nimos, pode ser necess√°rio adotar abordagens baseadas em tend√™ncias e popularidade.
        - Estrat√©gias como incentivo ao login podem melhorar a experi√™ncia do usu√°rio e a efic√°cia do modelo de recomenda√ß√£o.

        ‚úÖ **A√ß√µes recomendadas:**
        - Desenvolver estrat√©gias h√≠bridas: modelos personalizados para usu√°rios logados e recomenda√ß√µes baseadas em popularidade para an√¥nimos.
        - Criar incentivos para que mais usu√°rios fa√ßam login, como recomenda√ß√µes exclusivas ou conte√∫do personalizado.
        - Monitorar tend√™ncias que possam influenciar a taxa de login ao longo do tempo.
        ------------------------------------------------------------
        """
    )

    fig_bar_user_type = px.bar(
        user_type_time_df, x="date", y="count", color="userType",
        title="Evolu√ß√£o da Quantidade de Usu√°rios Logados vs. An√¥nimos",
        labels={"userType": "Tipo de Usu√°rio", "count": "Quantidade", "date": "Data"},
        barmode="stack",
        color_discrete_map=color_map
    )
    st.plotly_chart(fig_bar_user_type, use_container_width=True)

    st.markdown(
        """
        üìä **Gr√°fico 2: Evolu√ß√£o da Quantidade de Usu√°rios Logados vs. An√¥nimos (Barras Empilhadas)**

        üîé **Observa√ß√µes:**
        - O volume total de usu√°rios apresenta flutua√ß√µes ao longo do tempo.
        - O n√∫mero de usu√°rios an√¥nimos √© consistentemente maior do que o de usu√°rios logados.
        - Picos e quedas na atividade podem indicar eventos sazonais, mudan√ßas no tr√°fego do site ou fatores externos que influenciam o login.

        üßê **O que isso significa para o modelo de recomenda√ß√£o?**
        - A predomin√¢ncia de usu√°rios an√¥nimos refor√ßa a necessidade de recomenda√ß√µes baseadas em contexto, popularidade e tend√™ncias globais.
        - Eventos sazonais podem afetar padr√µes de login e intera√ß√£o, o que pode ser explorado para criar recomenda√ß√µes mais relevantes.
        - Not√≠cias mais acessadas durante a semana podem perder relev√¢ncia no final de semana, sugerindo que a rec√™ncia pode ter impacto diferenciado.

        ‚úÖ **A√ß√µes recomendadas:**
        - Investigar per√≠odos de picos e quedas para entender o que impulsiona o login dos usu√°rios.
        - Criar filtros ou categorias diferenciadas para recomenda√ß√µes baseadas em comportamento de usu√°rios logados vs. an√¥nimos.
        - Integrar dados temporais ao modelo para antecipar mudan√ßas no padr√£o de login e consumo de conte√∫do.
        ------------------------------------------------------------
        """
    )

    st.markdown(
        """
        üìå **Conclus√µes Finais**

        üöÄ **Impacto no modelo de recomenda√ß√£o:**

        - A segmenta√ß√£o entre usu√°rios logados e an√¥nimos √© essencial para ajustar estrat√©gias de recomenda√ß√£o.
        - Estrat√©gias h√≠bridas podem melhorar a experi√™ncia de usu√°rios com e sem login.
        - O incentivo ao login pode trazer benef√≠cios tanto para a personaliza√ß√£o quanto para o engajamento da plataforma.
        ------------------------------------------------------------
        """
    )

def show_analysis_6(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 6: Padr√µes de consumo de not√≠cias recentes vs. antigas</h1>", unsafe_allow_html=True)

    st.markdown("""
    üéØ **Objetivo**
    - Avaliar o impacto da rec√™ncia no consumo de not√≠cias.
    - Identificar se os usu√°rios tendem a acessar not√≠cias mais recentes ou se ainda h√° demanda por not√≠cias antigas.
    - Verificar se a rec√™ncia deve ser uma feature relevante para o modelo de recomenda√ß√£o.
    - Auxiliar na mitiga√ß√£o do problema de cold-start, propondo estrat√©gias para novos conte√∫dos.
    ------------------------------------------------------------
    """)

    recency_df = get_recency_data(spark)

    fig_hist_recency = px.histogram(
        recency_df, x="days_since_published", nbins=50,
        title="Distribui√ß√£o do Tempo Desde a Publica√ß√£o das Not√≠cias Acessadas",
        labels={"days_since_published": "Dias desde a Publica√ß√£o", "access_count": "N√∫mero de Acessos"}
    )
    st.plotly_chart(fig_hist_recency, use_container_width=True)

    st.markdown("""
    üìä **Gr√°fico 1: Distribui√ß√£o do Tempo Desde a Publica√ß√£o das Not√≠cias Acessadas**

    üîé **Observa√ß√µes:**
    - A distribui√ß√£o sugere que a maior parte das not√≠cias acessadas tem um tempo relativamente alto desde a publica√ß√£o.
    - No entanto, h√° um comportamento consistente de acesso ao longo do tempo, sem uma queda brusca.
    - Um pequeno n√∫mero de not√≠cias muito antigas ainda recebe visualiza√ß√µes.

    üßê **O que isso significa para o modelo?**
    - O consumo de not√≠cias n√£o se concentra apenas em conte√∫dos recentes, indicando que um modelo baseado apenas em rec√™ncia pode n√£o ser ideal.
    - Alguns conte√∫dos mais antigos podem continuar sendo relevantes, o que sugere que fatores como popularidade ou relev√¢ncia hist√≥rica podem ser √∫teis na recomenda√ß√£o.

    ‚úÖ **A√ß√µes Recomendadas:**
    - Criar um filtro de rec√™ncia adaptativo no modelo, priorizando not√≠cias novas, mas sem descartar completamente conte√∫dos mais antigos com alto engajamento.
    - Analisar se categorias espec√≠ficas (como pol√≠tica ou esportes) t√™m padr√µes de consumo diferentes.
    - Testar um peso de decaimento temporal para ajustar a import√¢ncia da rec√™ncia na recomenda√ß√£o.
    ------------------------------------------------------------
    """)

    time_recency_df = recency_df.groupby(["days_since_published"]).agg({"access_count": "sum"}).reset_index()

    fig_line_recency = px.line(
        time_recency_df, x="days_since_published", y="access_count",
        title="Tend√™ncia de Consumo de Not√≠cias Antigas vs. Recentes",
        labels={"days_since_published": "Dias desde a Publica√ß√£o", "access_count": "Total de Acessos"}
    )
    st.plotly_chart(fig_line_recency, use_container_width=True)

    st.markdown("""
    üìä **Gr√°fico 2: Tend√™ncia de Consumo de Not√≠cias Antigas vs. Recentes**

    üîé **Observa√ß√µes:**
    - H√° um pico massivo de consumo nos primeiros dias ap√≥s a publica√ß√£o da not√≠cia.
    - Ap√≥s esse per√≠odo inicial, o consumo cai drasticamente, indicando que a maioria dos usu√°rios busca conte√∫do recente.
    - No entanto, algumas not√≠cias antigas ainda aparecem com um pequeno volume de acessos residuais.

    üßê **O que isso significa para o modelo?**
    - Para um sistema de recomenda√ß√£o de not√≠cias, a rec√™ncia √© um fator cr√≠tico, mas n√£o absoluto.
    - Not√≠cias virais ou evergreen podem continuar sendo acessadas, exigindo um tratamento especial para evitar que o modelo descarte conte√∫dos importantes.
    - A baixa demanda por not√≠cias antigas sugere que o modelo deve dar menos peso a conte√∫dos mais antigos, mas sem remov√™-los completamente.

    ‚úÖ **A√ß√µes Recomendadas:**
    - Implementar um decaimento exponencial para priorizar not√≠cias novas, reduzindo a pontua√ß√£o de conte√∫dos antigos.
    - Criar uma feature de "vida √∫til da not√≠cia", identificando conte√∫dos que permanecem populares por mais tempo (como reportagens especiais ou investiga√ß√µes).
    - Ajustar o modelo para recomendar not√≠cias antigas apenas se houver relev√¢ncia contextual, como eventos hist√≥ricos relacionados a t√≥picos atuais.
    ------------------------------------------------------------
    """)

    st.markdown("""
    üìå **Conclus√µes Finais**

    üöÄ **Impacto no Modelo de Recomenda√ß√£o:**
    - O consumo de not√≠cias segue um padr√£o esperado: a maioria dos acessos ocorre logo ap√≥s a publica√ß√£o.
    - Para evitar o problema do cold-start, √© importante considerar rec√™ncia como um fator de recomenda√ß√£o, mas n√£o como √∫nico crit√©rio.
    - Algumas not√≠cias antigas ainda possuem valor, sugerindo a inclus√£o de um mecanismo para detectar conte√∫dos de longa relev√¢ncia.

    ‚úÖ **Pr√≥ximos Passos:**
    - Testar um modelo h√≠brido que combine rec√™ncia, popularidade e interesses do usu√°rio.
    - Avaliar diferentes categorias de not√≠cias para entender se h√° varia√ß√µes nos padr√µes de consumo.
    - Criar um sistema din√¢mico que adapte a import√¢ncia da rec√™ncia conforme o tipo de not√≠cia e o perfil do usu√°rio.
    ------------------------------------------------------------
    """)

def show_analysis_7(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 7: Sobreposi√ß√£o de acessos entre diferentes usu√°rios</h1>", unsafe_allow_html=True)
    
    st.markdown("""
    üéØ **Objetivo**
    - Identificar a distribui√ß√£o da popularidade das not√≠cias com base no n√∫mero de usu√°rios √∫nicos que as acessaram.
    - Avaliar se a maioria das not√≠cias √© consumida por um pequeno n√∫mero de usu√°rios ou se h√° um equil√≠brio na distribui√ß√£o.
    - Compreender a rela√ß√£o entre o n√∫mero de acessos e a exclusividade das not√≠cias, para auxiliar na personaliza√ß√£o do modelo de recomenda√ß√£o.
    ------------------------------------------------------------
    """)

    news_overlap_df = get_news_overlap_data(spark)

    fig_news_overlap = px.histogram(
        news_overlap_df, x="user_count", nbins=50,
        title="Distribui√ß√£o de Not√≠cias por N√∫mero de Usu√°rios",
        labels={"user_count": "N√∫mero de Usu√°rios √önicos", "count": "Quantidade de Not√≠cias"}
    )
    st.plotly_chart(fig_news_overlap, use_container_width=True)

    st.markdown("""
    üìä **Gr√°fico 1: Distribui√ß√£o de Not√≠cias por N√∫mero de Usu√°rios √önicos**

    üîé **Observa√ß√µes:**
    - A grande maioria das not√≠cias √© acessada por um n√∫mero muito pequeno de usu√°rios.
    - A distribui√ß√£o apresenta uma cauda longa, com poucas not√≠cias sendo altamente acessadas por muitos usu√°rios.
    - Isso sugere que a maior parte das not√≠cias tem um consumo nichado, sendo lida por um p√∫blico restrito.

    üßê **O que isso significa para o modelo?**
    - O modelo de recomenda√ß√£o pode precisar priorizar diferentes abordagens para not√≠cias populares e not√≠cias de nicho.
    - As not√≠cias consumidas por muitos usu√°rios podem ser recomendadas com base em tend√™ncias gerais.
    - J√° as not√≠cias de baixa popularidade podem exigir t√©cnicas de recomenda√ß√£o personalizadas, baseadas em prefer√™ncias individuais.

    ‚úÖ **A√ß√£o recomendada:**
    - Implementar um sistema h√≠brido que combine recomenda√ß√µes populares com recomenda√ß√µes personalizadas.
    - Avaliar a possibilidade de recomendar conte√∫dos menos acessados para expandir o engajamento do usu√°rio.
    ------------------------------------------------------------
    """)

    # Modificar a fun√ß√£o get_news_popularity_data para retornar j√° o DataFrame processado
    news_popularity_df = get_news_popularity_data(spark)
    
    # Criar bins usando pandas
    bins = pd.qcut(news_popularity_df["unique_users"], q=10, duplicates='drop')
    news_popularity_df["popularity_decile"] = bins.astype(str)

    # Contar not√≠cias por faixa de popularidade
    decile_counts = news_popularity_df.groupby("popularity_decile").size().reset_index(name="count")

    # Criar o gr√°fico
    fig = px.bar(
        decile_counts, x="popularity_decile", y="count",
        title="Distribui√ß√£o de Not√≠cias por Faixa de Popularidade (Decil)",
        labels={"popularity_decile": "N√∫mero de Usu√°rios √önicos (Decil)", "count": "Quantidade de Not√≠cias"},
        text_auto=True
    )

    fig.update_layout(
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=-0.3,
            xanchor="center",
            x=0.5
        )
    )

    st.plotly_chart(fig, use_container_width=True)

    st.markdown("""
    üìä **Gr√°fico 2: Distribui√ß√£o de Not√≠cias por Faixa de Popularidade**

    üîé **Observa√ß√µes**
    - A grande maioria das not√≠cias foi acessada por um n√∫mero muito pequeno de usu√°rios: mais de 210 mil not√≠cias foram vistas por no m√°ximo 10 usu√°rios.
    - Apenas um pequeno n√∫mero de not√≠cias ultrapassa 1000 usu√°rios √∫nicos, mostrando que poucas not√≠cias se tornam amplamente populares.
    - A curva segue um padr√£o de distribui√ß√£o de cauda longa, ou seja, poucas not√≠cias se tornam muito populares enquanto a grande maioria √© consumida por poucos usu√°rios.

    üßê **O que isso significa para o modelo de recomenda√ß√£o?**
    - O consumo de not√≠cias √© altamente desigual, com muitas mat√©rias tendo acessos extremamente baixos.
    - Um sistema de recomenda√ß√£o baseado apenas em popularidade pode n√£o ser ideal, pois grande parte das not√≠cias n√£o se torna viral.
    - Estrat√©gias como recomenda√ß√µes personalizadas (baseadas no perfil do usu√°rio) podem ser mais eficazes do que simplesmente promover as not√≠cias mais populares.
    - O cold-start para not√≠cias pouco acessadas pode ser um desafio, exigindo abordagens como recomenda√ß√£o baseada em conte√∫do.

    ‚úÖ **A√ß√µes Recomendadas**
    - Segmentar as not√≠cias: Criar estrat√©gias diferenciadas para conte√∫dos de nicho e conte√∫dos virais.
    - Considerar modelos h√≠bridos: Combinar recomenda√ß√µes baseadas em popularidade para usu√°rios casuais e recomenda√ß√µes personalizadas para usu√°rios frequentes.
    - Aprimorar a descoberta de not√≠cias menos acessadas, possivelmente atrav√©s de categoriza√ß√£o por temas e afinidade com o usu√°rio.
    - Explorar novas m√©tricas de relev√¢ncia, como tempo m√©dio na p√°gina, taxa de rolagem e cliques para identificar engajamento al√©m da popularidade.
    ------------------------------------------------------------
    """)

    st.markdown("""
    üìå **Conclus√µes Finais**

    üöÄ **Impacto no modelo de recomenda√ß√£o:**
    - A maioria das not√≠cias tem um p√∫blico pequeno e nichado, o que exige um sistema de recomenda√ß√£o capaz de identificar interesses espec√≠ficos de cada usu√°rio.
    - Modelos baseados apenas na popularidade podem n√£o ser a melhor abordagem para um sistema de recomenda√ß√£o eficiente neste cen√°rio.
    - Estrat√©gias h√≠bridas, combinando popularidade, personaliza√ß√£o e explora√ß√£o de novos conte√∫dos, podem ser a melhor alternativa para maximizar o engajamento dos usu√°rios.
    ------------------------------------------------------------
    """)


def show_analysis_8(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 8: Sobreposi√ß√£o de acessos entre diferentes usu√°rios</h1>", unsafe_allow_html=True)

    st.markdown("""
    üéØ **Objetivo**
    - Identificar a distribui√ß√£o do n√∫mero de usu√°rios √∫nicos por not√≠cia.
    - Verificar se a maioria das not√≠cias recebe poucos acessos ou se algumas s√£o amplamente consumidas.
    - Avaliar a viabilidade de recomendar not√≠cias populares vs. nichadas para diferentes perfis de usu√°rios.
    - Compreender o impacto da popularidade da not√≠cia na estrat√©gia de recomenda√ß√£o.
    ------------------------------------------------------------
    """)

    # Consulta SQL para obter a popularidade das not√≠cias
    news_popularity_df = spark.sql("""
        SELECT history AS news_id, COUNT(DISTINCT userId) AS unique_users
        FROM tab_treino
        GROUP BY history
    """)

    # Cria√ß√£o de bins para categorizar as not√≠cias por n√∫mero de usu√°rios √∫nicos
    news_popularity_df = news_popularity_df.withColumn(
        "user_bins",
        when(col("unique_users") <= 10, "1-10")
        .when((col("unique_users") > 10) & (col("unique_users") <= 100), "11-100")
        .when((col("unique_users") > 100) & (col("unique_users") <= 1000), "101-1000")
        .when((col("unique_users") > 1000) & (col("unique_users") <= 10000), "1001-10000")
        .otherwise("10001+")
    )

    # Contagem de not√≠cias por faixa de popularidade
    news_bins_count = news_popularity_df.groupBy("user_bins").count().toPandas()

    # Ordena√ß√£o das faixas de popularidade
    bins_order = ["1-10", "11-100", "101-1000", "1001-10000", "10001+"]
    news_bins_count["user_bins"] = pd.Categorical(news_bins_count["user_bins"], categories=bins_order, ordered=True)
    news_bins_count = news_bins_count.sort_values("user_bins")

    # Gr√°fico de barras para visualiza√ß√£o da distribui√ß√£o de not√≠cias por faixa de popularidade
    fig_bins_news = px.bar(
        news_bins_count, x="user_bins", y="count",
        title="Distribui√ß√£o de Not√≠cias por Faixa de Popularidade",
        labels={"user_bins": "N√∫mero de Usu√°rios √önicos", "count": "Quantidade de Not√≠cias"},
        text_auto=True
    )

    st.plotly_chart(fig_bins_news, use_container_width=True)

    # üìä **Gr√°fico: Distribui√ß√£o de Not√≠cias por Faixa de Popularidade**
    st.markdown("""
    üîé **Observa√ß√µes**
    - A maioria esmagadora das not√≠cias foi acessada por um n√∫mero muito pequeno de usu√°rios: mais de 210 mil not√≠cias foram vistas por no m√°ximo 10 usu√°rios.
    - Apenas um pequeno n√∫mero de not√≠cias ultrapassa 1000 usu√°rios √∫nicos, mostrando que poucas not√≠cias se tornam muito populares.
    - A curva segue um padr√£o de distribui√ß√£o de cauda longa, ou seja, poucas not√≠cias se tornam muito populares enquanto a grande maioria √© consumida por poucos usu√°rios.

    üßê **O que isso significa para o modelo de recomenda√ß√£o?**
    - O consumo de not√≠cias √© altamente desigual, com muitas mat√©rias tendo acessos extremamente baixos.
    - Um sistema de recomenda√ß√£o baseado apenas em popularidade pode n√£o ser ideal, pois grande parte das not√≠cias n√£o se torna viral.
    - Estrat√©gias como recomenda√ß√µes personalizadas (baseadas no perfil do usu√°rio) podem ser mais eficazes do que simplesmente promover as not√≠cias mais populares.
    - O cold-start para not√≠cias pouco acessadas pode ser um desafio, exigindo abordagens como recomenda√ß√£o baseada em conte√∫do.

    ‚úÖ **A√ß√µes Recomendadas**
    - Segmentar as not√≠cias: Criar estrat√©gias diferenciadas para conte√∫dos de nicho e conte√∫dos virais.
    - Considerar modelos h√≠bridos: Combinar recomenda√ß√µes baseadas em popularidade para usu√°rios casuais e recomenda√ß√µes personalizadas para usu√°rios frequentes.
    - Aprimorar a descoberta de not√≠cias menos acessadas, possivelmente atrav√©s de categoriza√ß√£o por temas e afinidade com o usu√°rio.
    - Explorar novas m√©tricas de relev√¢ncia, como tempo m√©dio na p√°gina, taxa de rolagem e cliques para identificar engajamento al√©m da popularidade.
    ------------------------------------------------------------
    """)

    st.markdown("""
    üìå **Conclus√µes Finais**            
    üöÄ **Impacto no modelo de recomenda√ß√£o:**
    - A distribui√ß√£o desigual do consumo de not√≠cias sugere que a popularidade sozinha n√£o pode ser o √∫nico crit√©rio para recomenda√ß√µes eficazes.
    - Para maximizar a relev√¢ncia, o modelo deve equilibrar a recomenda√ß√£o de not√≠cias populares e menos conhecidas, dependendo do perfil do usu√°rio.
    - Not√≠cias menos acessadas ainda podem ser valiosas para p√∫blicos espec√≠ficos, refor√ßando a necessidade de personaliza√ß√£o.
    ------------------------------------------------------------
    """)

def show_analysis_9(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 9: Calcular a correla√ß√£o entre n√∫mero de cliques e tempo de leitura</h1>", unsafe_allow_html=True)
    
    st.markdown("""
    üéØ **Objetivo**
    - Identificar se h√° uma rela√ß√£o direta entre o n√∫mero de cliques e o tempo de leitura.
    - Avaliar se um maior n√∫mero de cliques significa que o usu√°rio passou mais tempo na p√°gina.
    - Compreender se o tempo de perman√™ncia pode ser um indicador de engajamento para o modelo de recomenda√ß√£o.
    - Detectar poss√≠veis padr√µes ou outliers que possam impactar a modelagem.
    ------------------------------------------------------------
    """)

    correlation, sample_df = get_clicks_time_correlation(spark)
    st.write(f"Correla√ß√£o entre n√∫mero de cliques e tempo de leitura: {correlation:.4f}")

    # Gr√°fico de dispers√£o para visualiza√ß√£o da correla√ß√£o
    fig_clicks_time = px.scatter(
        sample_df, x="numberOfClicksHistory", y="timeOnPageHistory",
        title="Correla√ß√£o entre N√∫mero de Cliques e Tempo de Leitura",
        labels={"numberOfClicksHistory": "N√∫mero de Cliques", "timeOnPageHistory": "Tempo de Leitura (ms)"},
        trendline="ols",
        trendline_color_override="red"
    )

    st.plotly_chart(fig_clicks_time, use_container_width=True)

    st.markdown("""
    üìä **Gr√°fico: Correla√ß√£o entre N√∫mero de Cliques e Tempo de Leitura**
    
    Este gr√°fico de dispers√£o mostra a rela√ß√£o entre o n√∫mero de cliques em uma not√≠cia e o tempo que os usu√°rios passam lendo a p√°gina.
    
    üîé **Observa√ß√µes**
    - H√° uma tend√™ncia de correla√ß√£o positiva fraca entre cliques e tempo de leitura, como indicado pela linha de tend√™ncia em vermelho.
    - Muitos pontos est√£o concentrados pr√≥ximos da origem, sugerindo que a maioria das intera√ß√µes ocorre em um curto per√≠odo de tempo, independentemente da quantidade de cliques.
    - Existem outliers extremos, onde alguns usu√°rios passam um tempo muito alto na p√°gina, independentemente do n√∫mero de cliques.
    - O comportamento sugere que nem sempre um maior n√∫mero de cliques resulta em um tempo de leitura significativamente maior.
    
    üßê **O que isso significa para o modelo de recomenda√ß√£o?**
    
    - O tempo de leitura isoladamente pode n√£o ser um preditor confi√°vel de engajamento, j√° que usu√°rios podem abrir a p√°gina e n√£o necessariamente consumi-la por completo.
    - Recomenda√ß√µes baseadas somente no n√∫mero de cliques podem n√£o capturar corretamente o n√≠vel de interesse do usu√°rio.
    - Estrat√©gias h√≠bridas devem ser exploradas, combinando cliques, tempo de leitura, e outras m√©tricas como taxa de rolagem e n√∫mero de visitas repetidas.
    
    ‚úÖ **A√ß√µes Recomendadas**
    
    - Filtrar outliers: Excluir registros de tempo de leitura anormalmente altos ou baixos para evitar distor√ß√µes nas m√©tricas.
    - Incorporar m√©tricas complementares: Al√©m de cliques e tempo de leitura, utilizar taxa de rolagem e intera√ß√µes subsequentes como indicativos de interesse.
    - Analisar padr√µes de engajamento: Segmentar usu√°rios em grupos de comportamento (r√°pido consumo, leitura profunda, baixa intera√ß√£o) para personalizar recomenda√ß√µes.
    ------------------------------------------------------------
    """)

    st.markdown("""
    üìå **Conclus√µes Finais**
    
    üöÄ **Impacto no modelo de recomenda√ß√£o:**
    
    - Embora haja uma leve correla√ß√£o entre n√∫mero de cliques e tempo de leitura, ela n√£o √© forte o suficiente para ser usada isoladamente como um indicador de interesse.
    - O modelo pode ser aprimorado ao considerar m√©tricas combinadas de engajamento, como tempo m√©dio na p√°gina + taxa de rolagem + intera√ß√µes repetidas.
    - Estrat√©gias personalizadas podem ser √∫teis para diferenciar leituras r√°pidas de usu√°rios casuais e consumo aprofundado de usu√°rios altamente engajados.
    ------------------------------------------------------------
    """)

def show_analysis_10(spark):
    st.markdown("<h1 style='font-size: 32px;'>An√°lise 10: Perfis de Usu√°rios com Base na Quantidade de Intera√ß√µes</h1>", unsafe_allow_html=True)

    st.markdown("""
    üéØ **Objetivo**
    - Identificar a distribui√ß√£o de usu√°rios com base na quantidade de intera√ß√µes.
    - Determinar se existe uma rela√ß√£o entre alta/baixa intera√ß√£o e padr√µes de consumo.
    - Avaliar a necessidade de segmenta√ß√£o de usu√°rios para estrat√©gias de recomenda√ß√£o diferenciadas.
    - Detectar poss√≠veis outliers, como bots ou heavy users, que podem impactar a recomenda√ß√£o.
    ------------------------------------------------------------
    """)

    # Criar a distribui√ß√£o dos usu√°rios por n√∫mero de intera√ß√µes
    user_interactions_df = get_user_interactions_data(spark)

    # Calcular os decis da distribui√ß√£o de intera√ß√µes e remover duplicatas
    deciles = np.percentile(user_interactions_df["interaction_count"], np.arange(0, 110, 10))
    deciles = np.unique(deciles)  # Remove valores duplicados

    # Criar labels para cada decil
    decile_labels = [f"{int(deciles[i])}-{int(deciles[i+1])}" for i in range(len(deciles)-1)]

    # Criar nova coluna categorizando usu√°rios pelos decis
    user_interactions_df["interaction_decile"] = pd.cut(
        user_interactions_df["interaction_count"], 
        bins=deciles, 
        labels=decile_labels, 
        include_lowest=True
    )

    # Contagem de usu√°rios por faixa de decil
    decile_counts = user_interactions_df["interaction_decile"].value_counts().sort_index().reset_index()
    decile_counts.columns = ["Interaction Range", "User Count"]

    # Calcular percentual acumulado
    decile_counts["Cumulative %"] = (decile_counts["User Count"].cumsum() / decile_counts["User Count"].sum()) * 100

    # Gr√°fico de barras e linha de percentual acumulado
    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=decile_counts["Interaction Range"], 
        y=decile_counts["User Count"], 
        name="Usu√°rios por Faixa",
        text=decile_counts["User Count"], 
        textposition="outside"
    ))

    fig.add_trace(go.Scatter(
        x=decile_counts["Interaction Range"], 
        y=decile_counts["Cumulative %"], 
        name="Percentual Acumulado",
        mode="lines+markers",
        yaxis="y2",
        line=dict(color='red')  # Adicionando a cor vermelha
    ))

    fig.update_layout(
        title="Distribui√ß√£o de Usu√°rios por Faixa de Intera√ß√£o",
        xaxis_title="Faixa de Intera√ß√µes",
        yaxis_title="Quantidade de Usu√°rios",
        yaxis2=dict(
            title="Percentual Acumulado",
            overlaying="y",
            side="right",
            showgrid=False
        ),
        legend=dict(
            orientation="h",  
            yanchor="bottom", 
            y=-0.3, 
            xanchor="center", 
            x=0.5
        ),
        bargap=0.1,
        showlegend=True
    )

    st.plotly_chart(fig, use_container_width=True)

    st.markdown("""
    üìä **An√°lise do Gr√°fico: Distribui√ß√£o de Usu√°rios por Faixa de Intera√ß√£o**
    
    üîé **Observa√ß√µes:**
    
    - A grande maioria dos usu√°rios (aproximadamente 342 mil) possui apenas 1 a 2 intera√ß√µes, indicando um consumo extremamente espor√°dico.
    - A queda no n√∫mero de usu√°rios conforme o n√≠vel de intera√ß√£o aumenta √© bastante acentuada:
        - 2-3 intera√ß√µes: 40 mil usu√°rios.
        - 3-4 intera√ß√µes: 24 mil usu√°rios.
        - 4-10 intera√ß√µes: 57 mil usu√°rios.
        - 10-35 intera√ß√µes: 54 mil usu√°rios.
        - 35+ intera√ß√µes: 57 mil usu√°rios.
    - O gr√°fico mostra um comportamento de cauda longa, onde poucos usu√°rios possuem alta frequ√™ncia de intera√ß√µes, enquanto a maioria interage muito pouco com a plataforma.
    - O percentual acumulado refor√ßa essa assimetria: uma pequena fra√ß√£o dos usu√°rios representa a maior parte das intera√ß√µes.
    
    üßê **O que isso significa para o modelo?**
    
    - Usu√°rios casuais (1-2 intera√ß√µes) representam a maioria, sugerindo que estrat√©gias de recomenda√ß√£o devem focar em conte√∫dos populares para engaj√°-los.
    - Usu√°rios moderadamente ativos (4-10 intera√ß√µes) podem se beneficiar de um modelo h√≠brido, que combina recomenda√ß√µes baseadas em popularidade e hist√≥rico de consumo.
    - Usu√°rios altamente engajados (35+ intera√ß√µes) devem receber recomenda√ß√µes altamente personalizadas, considerando seu hist√≥rico detalhado.
    - A presen√ßa de usu√°rios com centenas ou milhares de intera√ß√µes pode indicar heavy users ou at√© mesmo bots, que devem ser analisados separadamente.
    
    ‚úÖ **A√ß√µes Recomendadas:**
    
    1. **Segmenta√ß√£o de Perfis de Usu√°rios:**
        - Criar modelos diferenciados para usu√°rios casuais, moderadamente ativos e altamente engajados.
        - Para usu√°rios casuais: recomenda√ß√µes baseadas em not√≠cias populares e tend√™ncias.
        - Para usu√°rios frequentes: recomenda√ß√µes baseadas no comportamento hist√≥rico e padr√µes de similaridade.
    
    2. **Detec√ß√£o de Outliers:**
    
        - Identificar e analisar usu√°rios com um n√∫mero excepcionalmente alto de intera√ß√µes, evitando distor√ß√µes na recomenda√ß√£o.
        - Verificar a exist√™ncia de bots ou acessos automatizados.
    
    3. **Ajuste no Modelo de Recomenda√ß√£o:**
    
        - Introduzir pesos diferenciados para os diferentes segmentos de usu√°rios.
        - Testar abordagens que incentivem usu√°rios casuais a aumentar seu engajamento.
    
    4. **An√°lise Temporal:**
    
        - Avaliar padr√µes sazonais para entender se o engajamento dos usu√°rios varia ao longo do tempo.
        - Identificar momentos de pico que podem ser explorados para melhorar a personaliza√ß√£o da recomenda√ß√£o.
    ------------------------------------------------------------
    """)

    st.markdown("""
    üìå **Conclus√µes Finais**
    
    üöÄ **Impacto no modelo de recomenda√ß√£o:**
    
    - A distribui√ß√£o altamente desigual do n√∫mero de intera√ß√µes refor√ßa a necessidade de segmenta√ß√£o de usu√°rios no sistema de recomenda√ß√£o.
    - Usu√°rios casuais devem receber sugest√µes de conte√∫dos populares para aumentar sua intera√ß√£o inicial.
    - Usu√°rios altamente engajados precisam de um modelo personalizado que aproveite seu comportamento detalhado.
    - Estrat√©gias de filtragem de bots e an√°lise de heavy users podem ajudar a evitar distor√ß√µes nas recomenda√ß√µes.
    - Ajustes din√¢micos na recomenda√ß√£o, considerando padr√µes temporais, podem tornar o sistema mais eficiente e responsivo.
    ------------------------------------------------------------
    """)

def show_general_eda_conclusion(spark):
    st.markdown("""
    ------------------------------------------------------------
    """)
    st.markdown("<h1 style='font-size: 32px;'>üìå Conclus√£o Final da An√°lise Explorat√≥ria de Dados</h1>", unsafe_allow_html=True)
    st.markdown("""
    üéØ Objetivo
    - A an√°lise explorat√≥ria buscou entender os padr√µes de consumo de not√≠cias no G1, identificando fatores que impactam a personaliza√ß√£o das recomenda√ß√µes. O desafio principal envolve equilibrar rec√™ncia, engajamento e popularidade para diferentes perfis de usu√°rios.

    -----------------
    
    üìä Principais Descobertas e Impacto no Modelo
    
    1Ô∏è‚É£ **Padr√£o de Consumo e Segmenta√ß√£o de Usu√°rios**
    - A maioria dos usu√°rios l√™ poucas not√≠cias, enquanto um pequeno grupo consome intensamente.
    - O comportamento varia entre usu√°rios casuais, recorrentes e altamente engajados.
    
    ‚úÖ **A√ß√µes Recomendadas:**
    - Implementar modelos diferenciados para cada perfil.
    - Criar estrat√©gias para detectar e tratar bots.
    - Testar recomenda√ß√µes h√≠bridas: personaliza√ß√£o para usu√°rios frequentes, tend√™ncias para casuais.
    
    -----------------
    
    2Ô∏è‚É£ **Rec√™ncia vs. Popularidade**
    - A maioria das not√≠cias tem alto consumo logo ap√≥s a publica√ß√£o, mas algumas continuam relevantes ao longo do tempo.
    
    ‚úÖ **A√ß√µes Recomendadas:**
    - Implementar um peso adaptativo para a rec√™ncia, ajustando conforme a categoria da not√≠cia.
    - Detectar conte√∫dos atemporais e evitar descart√°-los prematuramente.
    - Explorar um modelo h√≠brido (rec√™ncia + relev√¢ncia + engajamento).
    
    -----------------
    
    3Ô∏è‚É£ **Engajamento e Tempo na P√°gina**
    - O tempo de leitura isoladamente n√£o indica alto engajamento.
    - Cliques, tempo de leitura e taxa de rolagem combinados s√£o melhores indicadores.
    
    ‚úÖ **A√ß√µes Recomendadas:**
    - Criar um score de engajamento que combine m√©tricas de intera√ß√£o.
    - Filtrar outliers (sess√µes anormais).
    - Avaliar padr√µes distintos de leitura para ajustar recomenda√ß√µes.
    
    -----------------
    
    4Ô∏è‚É£ **Retorno e Fideliza√ß√£o**
    - Usu√°rios apresentam padr√µes c√≠clicos de acesso, influenciados por eventos sazonais.
    
    ‚úÖ **A√ß√µes Recomendadas:**
    - Ajustar recomenda√ß√µes conforme hor√°rio e dia da semana.
    - Criar um sistema din√¢mico de reten√ß√£o, incentivando novas visitas.
    - Monitorar o impacto de not√≠cias de grande repercuss√£o no engajamento.
    
    -----------------
    
    5Ô∏è‚É£ **Usu√°rios Logados vs. An√¥nimos**
    - A maioria dos acessos ocorre sem login, reduzindo a personaliza√ß√£o poss√≠vel.
    
    ‚úÖ **A√ß√µes Recomendadas:**
    - Recomenda√ß√£o h√≠brida: personalizada para logados, baseada em popularidade para an√¥nimos.
    - Criar incentivos ao login, aumentando a base de dados hist√≥ricos.
    
    -----------------
    
    6Ô∏è‚É£ **Popularidade e Explora√ß√£o de Not√≠cias**
    - Poucas not√≠cias dominam a aten√ß√£o, enquanto a maioria tem baixa visibilidade.
    - O consumo segue um padr√£o de cauda longa.
    
    ‚úÖ **A√ß√µes Recomendadas:**
    - Balancear recomenda√ß√µes entre tend√™ncias e conte√∫dos de nicho.
    - Explorar recomenda√ß√µes por similaridade para expandir o consumo de not√≠cias menos acessadas.
    - Considerar uma abordagem que priorize recomenda√ß√µes de not√≠cias novas, visto que not√≠cias antigas, mesmo que populares, podem perder relev√¢ncia.
    
    -----------------
    
    üèó Impacto nas Features de Engenharia
    
    | **Feature**             | **Justificativa** |
    |-------------------------|------------------------------------------|
    | **Rec√™ncia**            | Influ√™ncia direta na relev√¢ncia.         |
    | **Popularidade**        | Algumas not√≠cias continuam relevantes por mais tempo. |
    | **Engajamento do usu√°rio** | Tempo na p√°gina, cliques e rolagem s√£o mais √∫teis quando combinados. |
    | **Padr√£o de consumo**   | Usu√°rios casuais e recorrentes precisam de abordagens diferentes. |
    | **Hor√°rio e sazonalidade** | O consumo varia ao longo do dia e semana. |
    
    .
                
    ‚úÖ **Pr√≥ximos Passos:**
    - Refinar modelo h√≠brido (TF-IDF + LightFM) com pesos ajust√°veis para rec√™ncia e engajamento.
    - Implementar estrat√©gias de reten√ß√£o e descoberta de conte√∫dos.
    - Desenvolver um sistema adapt√°vel a eventos sazonais e prefer√™ncias individuais.
    
    -----------------
    
    üöÄ **Conclus√£o Final**
    - A an√°lise revelou que a rec√™ncia √© essencial, mas deve ser equilibrada com popularidade e engajamento. A segmenta√ß√£o de usu√°rios e o uso de modelos h√≠bridos s√£o essenciais para uma recomenda√ß√£o eficiente. O pr√≥ximo passo √© testar diferentes abordagens, ajustando os pesos das features para otimizar a precis√£o e a experi√™ncia do usu√°rio.
    
    üîπ **Resumo Final:**
    - Modelos distintos para usu√°rios casuais e recorrentes.
    - Rec√™ncia como fator cr√≠tico, mas n√£o √∫nico.
    - Engajamento medido por m√∫ltiplas m√©tricas combinadas.
    - Abordagem h√≠brida para equilibrar popularidade e personaliza√ß√£o.
    """)

def pre_cache_all_data(spark):
    """Fun√ß√£o para pr√©-cachear todos os dados"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_functions = 10  # N√∫mero total de fun√ß√µes para cache
    cached_functions = 0

    try:
        # Lista de tuplas (fun√ß√£o, descri√ß√£o)
        cache_functions = [
            (get_news_per_user, "Cacheando dados de distribui√ß√£o de leituras..."),
            (get_time_distribution, "Cacheando dados de distribui√ß√£o temporal..."),
            (get_engagement_data, "Cacheando dados de engajamento..."),
            (get_retention_data, "Cacheando dados de reten√ß√£o..."),
            (get_user_type_data, "Cacheando dados de tipos de usu√°rio..."),
            (get_recency_data, "Cacheando dados de rec√™ncia..."),
            (get_news_overlap_data, "Cacheando dados de sobreposi√ß√£o..."),
            (get_news_popularity_data, "Cacheando dados de popularidade..."),
            (get_clicks_time_correlation, "Cacheando dados de correla√ß√£o..."),
            (get_user_interactions_data, "Cacheando dados de intera√ß√µes...")
        ]

        for func, description in cache_functions:
            status_text.text(description)
            func(spark)  # Executa a fun√ß√£o de cache
            cached_functions += 1
            progress_bar.progress(cached_functions / total_functions)
            time.sleep(0.5)  # Pequena pausa para visualiza√ß√£o

        progress_bar.progress(1.0)
        status_text.text("‚úÖ Todos os dados foram cacheados com sucesso!")
        return True

    except Exception as e:
        status_text.text(f"‚ùå Erro ao cachear dados: {str(e)}")
        return False

def show_home():
    st.title("üìä An√°lise Explorat√≥ria - Sistema de Recomenda√ß√£o G1")
    
    st.markdown("""
    ### üéØ Sobre o Projeto
    
    Esta an√°lise explorat√≥ria faz parte do desenvolvimento de um sistema de recomenda√ß√£o de not√≠cias para o portal G1, 
    um dos maiores portais de not√≠cias do Brasil. O objetivo √© compreender profundamente os padr√µes de consumo de 
    not√≠cias dos usu√°rios para criar recomenda√ß√µes mais precisas e personalizadas.
    
    ### üìå Principais Aspectos Analisados
    
    - **Comportamento dos Usu√°rios**: Padr√µes de leitura, hor√°rios de acesso e engajamento
    - **Consumo de Conte√∫do**: Distribui√ß√£o de leituras, popularidade das not√≠cias e rec√™ncia
    - **Perfis de Usu√°rio**: Segmenta√ß√£o entre usu√°rios casuais e frequentes
    - **M√©tricas de Engajamento**: Tempo de leitura, cliques e intera√ß√µes
    
    ### üìä Base de Dados
    
    - **Per√≠odo Analisado**: Dados hist√≥ricos de intera√ß√µes dos usu√°rios com not√≠cias do G1
    - **Volume de Dados**: Milh√µes de intera√ß√µes processadas
    - **M√©tricas Coletadas**: Tempo de leitura, cliques, scroll, tipo de usu√°rio e outros indicadores
    
    ### üéØ Objetivo do Sistema de Recomenda√ß√£o
    
    Desenvolver um sistema capaz de:
    - Personalizar recomenda√ß√µes com base no perfil do usu√°rio
    - Equilibrar not√≠cias populares e conte√∫do personalizado
    - Considerar a temporalidade das not√≠cias
    - Melhorar o engajamento dos usu√°rios com o portal
    """)

    # Criar containers tempor√°rios para as mensagens de inicializa√ß√£o
    loading_message = st.empty()
    loading_message.markdown("#### üöÄ Inicializando o Sistema\nAguarde enquanto preparamos os dados para uma experi√™ncia mais r√°pida...")

    # Tenta inicializar o Spark e cachear os dados
    try:
        spark = init_spark()
        treino, itens = load_data(spark)
        treino.createOrReplaceTempView("tab_treino")
        itens.createOrReplaceTempView("tab_itens")

        cache_status = pre_cache_all_data(spark)

        if cache_status:
            success_message = st.empty()
            success_message.success("#### ‚úÖ Sistema Pronto!")
            # Aguarda 2 segundos e limpa as mensagens
            time.sleep(2)
            loading_message.empty()
            success_message.empty()
        else:
            st.warning("‚ö†Ô∏è Alguns dados podem demorar para carregar durante a navega√ß√£o.")

    except Exception as e:
        st.error(f"‚ùå Erro ao inicializar o sistema: {str(e)}")

def main():
    # Sidebar estilizada
    with st.sidebar:
        st.title("An√°lises")
        
        # Menu estilizado com √≠cones - Tema Escuro
        analysis_option = option_menu(
            menu_title=None,
            options=[
                "Home",
                "An√°lise 1: Distribui√ß√£o de Leituras",
                "An√°lise 2: Distribui√ß√£o Temporal",
                "An√°lise 3: Rela√ß√£o Tempo e Engajamento",
                "An√°lise 4: Taxa de Retorno",
                "An√°lise 5: Usu√°rios Logados vs An√¥nimos",
                "An√°lise 6: Padr√µes de Consumo",
                "An√°lise 7: Sobreposi√ß√£o de Acessos",
                "An√°lise 8: Distribui√ß√£o de Usu√°rios",
                "An√°lise 9: Correla√ß√µes",
                "An√°lise 10: Perfis de Usu√°rios",
                "Conclus√µes Gerais"
            ],
            icons=[
                "house", "bar-chart-line", "clock-history", "graph-up-arrow",
                "people", "file-earmark-text", "map", "person-lines-fill",
                "link", "person-bounding-box", "check-circle"
            ],
            menu_icon="cast",
            default_index=0,
            styles={
                "container": {"padding": "5px", "background-color": "#262730"},
                "icon": {"color": "#ffffff", "font-size": "18px"}, 
                "nav-link": {
                    "font-size": "16px",
                    "text-align": "left",
                    "margin": "2px",
                    "padding": "10px",
                    "color": "#ffffff",
                    "--hover-color": "#363c4c"
                },
                "nav-link-selected": {
                    "background-color": "#ff5e5e",
                    "font-weight": "bold",
                    "color": "white"
                },
                "menu-title": {
                    "color": "#ffffff",
                    "font-size": "20px",
                    "font-weight": "bold"
                }
            }
        )
        
        # For√ßar recarregamento ao mudar de p√°gina
        if st.session_state.get("last_page") != analysis_option:
            st.session_state["last_page"] = analysis_option
            st.experimental_rerun()

    # Inicializa Spark
    try:
        spark = init_spark()
        treino, itens = load_data(spark)
        treino.createOrReplaceTempView("tab_treino")
        itens.createOrReplaceTempView("tab_itens")

        # Chamando a an√°lise correspondente
        analysis_functions = {
            "An√°lise 1: Distribui√ß√£o de Leituras": show_analysis_1,
            "An√°lise 2: Distribui√ß√£o Temporal": show_analysis_2,
            "An√°lise 3: Rela√ß√£o Tempo e Engajamento": show_analysis_3,
            "An√°lise 4: Taxa de Retorno": show_analysis_4,
            "An√°lise 5: Usu√°rios Logados vs An√¥nimos": show_analysis_5,
            "An√°lise 6: Padr√µes de Consumo": show_analysis_6,
            "An√°lise 7: Sobreposi√ß√£o de Acessos": show_analysis_7,
            "An√°lise 8: Distribui√ß√£o de Usu√°rios": show_analysis_8,
            "An√°lise 9: Correla√ß√µes": show_analysis_9,
            "An√°lise 10: Perfis de Usu√°rios": show_analysis_10,
            "Conclus√µes Gerais": show_general_eda_conclusion,
        }

        if analysis_option == "Home":
            show_home()
        elif analysis_option in analysis_functions:
            analysis_functions[analysis_option](spark)
        else:
            st.write("üè† Bem-vindo! Escolha uma an√°lise no menu lateral.")

    except Exception as e:
        st.error(f"Erro ao carregar os dados: {str(e)}")

if __name__ == "__main__":
    main()

