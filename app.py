import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pyspark.sql import SparkSession
import os
import logging

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configura√ß√£o de tema e cores
THEME_COLORS = {
    'primary': '#2196F3',
    'secondary': '#FF9800',
    'background': '#000000',
    'surface': '#1E1E1E',   
    'text': '#FFFFFF'       
}

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Dashboard G1 - Sistema de Recomenda√ß√£o",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado
st.markdown("""
    <style>
    .main {
        background-color: #000000;  /* Fundo preto */
        color: #FFFFFF;              /* Texto branco */
    }
    .stMetric {
        background-color: #1E1E1E;   /* Cor de superf√≠cie */
        color: #FFFFFF;              /* Texto branco */
    }
    </style>
""", unsafe_allow_html=True)

# Inicializar Spark e carregar dados
@st.cache_resource
def init_spark():
    spark = SparkSession.builder \
        .appName("G1 Recommendations") \
        .config("spark.sql.parquet.datetimeRebaseModeInRead", "CORRECTED") \
        .config("spark.sql.parquet.datetimeRebaseModeInWrite", "CORRECTED") \
        .getOrCreate()
    
    # Ler os arquivos parquet com particionamento
    treino = spark.read \
        .option("mergeSchema", "false") \
        .parquet("/app/datalake/silver/treino") \
    
    itens = spark.read \
    .option("mergeSchema", "false") \
    .parquet("datalake/silver/itens") \
    
    # Registrar as tabelas tempor√°rias
    treino.createOrReplaceTempView("tab_treino")
    itens.createOrReplaceTempView("tab_itens")
    
    return spark

# Cache para queries frequentes
@st.cache_data
def get_basic_metrics():
    return {
        "total_users": spark.sql('SELECT COUNT(DISTINCT userId) FROM tab_treino').collect()[0][0],
        "total_interactions": spark.sql('SELECT COUNT(*) FROM tab_treino').collect()[0][0],
        "avg_interactions": spark.sql('SELECT CAST(COUNT(*)/COUNT(DISTINCT userId) AS INT) FROM tab_treino').collect()[0][0],
        "total_news": spark.sql('SELECT COUNT(DISTINCT history) FROM tab_treino').collect()[0][0]
    }

@st.cache_data
def get_user_distribution():
    return spark.sql("""
        SELECT 
            userType,
            COUNT(DISTINCT userId) as unique_users,
            CAST(AVG(numberOfClicksHistory) as INT) as avg_clicks,
            CAST(AVG(timeOnPageHistory) as INT) as avg_time,
            CAST(AVG(scrollPercentageHistory) as INT) as avg_scroll
        FROM tab_treino
        GROUP BY userType
        ORDER BY unique_users DESC
    """).toPandas()

@st.cache_data
def get_cold_start_analysis(_spark):
    return _spark.sql("""
        WITH user_interactions AS (
            SELECT userId, COUNT(*) as interaction_count
            FROM tab_treino
            GROUP BY userId
        )
        SELECT 
            CASE 
                WHEN interaction_count < 5 THEN 'Muito Baixo (< 5)'
                WHEN interaction_count < 10 THEN 'Baixo (5-9)'
                WHEN interaction_count < 20 THEN 'M√©dio (10-19)'
                ELSE 'Alto (20+)'
            END as nivel_interacao,
            COUNT(*) as num_users,
            CAST((COUNT(*) * 100.0 / SUM(COUNT(*)) OVER ()) as INT) as percentual
        FROM user_interactions
        GROUP BY 
            CASE 
                WHEN interaction_count < 5 THEN 'Muito Baixo (< 5)'
                WHEN interaction_count < 10 THEN 'Baixo (5-9)'
                WHEN interaction_count < 20 THEN 'M√©dio (10-19)'
                ELSE 'Alto (20+)'
            END
        ORDER BY num_users DESC
    """).toPandas()

@st.cache_data
def get_recency_analysis():
    return spark.sql("""
        WITH user_recency AS (
            SELECT 
                userId,
                DATEDIFF(MAX(timestampHistory), MIN(timestampHistory)) as days_active
            FROM tab_treino
            GROUP BY userId
        )
        SELECT 
            CASE 
                WHEN days_active <= 1 THEN '1 dia'
                WHEN days_active <= 7 THEN '1 semana'
                WHEN days_active <= 30 THEN '1 m√™s'
                ELSE 'Mais de 1 m√™s'
            END as periodo,
            COUNT(*) as num_users
        FROM user_recency
        GROUP BY 
            CASE 
                WHEN days_active <= 1 THEN '1 dia'
                WHEN days_active <= 7 THEN '1 semana'
                WHEN days_active <= 30 THEN '1 m√™s'
                ELSE 'Mais de 1 m√™s'
            END
        ORDER BY num_users DESC
    """).toPandas()

@st.cache_data
def get_content_analysis():
    return spark.sql("""
        SELECT 
            i.page as category,
            COUNT(*) as total_reads,
            COUNT(DISTINCT t.userId) as unique_readers,
            CAST(AVG(t.timeOnPageHistory) as INT) as avg_time
        FROM tab_treino t
        JOIN tab_itens i ON t.history = i.page
        GROUP BY i.page
        HAVING COUNT(*) > 100
        ORDER BY total_reads DESC
        LIMIT 10
    """).toPandas()

@st.cache_data
def get_temporal_distribution(_spark):
    try:
        temporal_dist = _spark.sql("""
            SELECT 
                DATE(timestampHistory) as data,
                COUNT(*) as total_interacoes,
                COUNT(DISTINCT userId) as usuarios_unicos
            FROM tab_treino
            GROUP BY DATE(timestampHistory)
            ORDER BY data
        """).toPandas()
        
        logger.info(f"Dados de distribui√ß√£o temporal: {temporal_dist.head()}")
        return temporal_dist
    except Exception as e:
        logger.error(f"Erro ao obter distribui√ß√£o temporal: {str(e)}")
        return pd.DataFrame()  # Retorna um DataFrame vazio

@st.cache_data
def get_engagement_metrics():
    return spark.sql("""
        SELECT 
            userType,
            CAST(AVG(numberOfClicksHistory) as INT) as media_clicks,
            CAST(AVG(timeOnPageHistory)/60 as INT) as media_tempo_minutos,
            CAST(AVG(scrollPercentageHistory) as INT) as media_scroll
        FROM tab_treino
        GROUP BY userType
        ORDER BY media_tempo_minutos DESC
    """).toPandas()

@st.cache_data
def get_hourly_pattern():
    return spark.sql("""
        SELECT 
            HOUR(timestampHistory) as hora,
            COUNT(*) as total_acessos,
            COUNT(DISTINCT userId) as usuarios_unicos
        FROM tab_treino
        GROUP BY HOUR(timestampHistory)
        ORDER BY hora
    """).toPandas()

@st.cache_data
def get_top_categories():
    return spark.sql("""
        SELECT 
            i.page as category,
            COUNT(*) as total_reads,
            COUNT(DISTINCT t.userId) as unique_readers,
            CAST(AVG(t.timeOnPageHistory)/60 as INT) as avg_time_minutes,
            CAST(AVG(t.scrollPercentageHistory) as INT) as avg_scroll
        FROM tab_treino t
        JOIN tab_itens i ON t.history = i.page
        GROUP BY i.page
        HAVING COUNT(*) > 100
        ORDER BY total_reads DESC
        LIMIT 10
    """).toPandas()

@st.cache_data
def get_correlation_metrics():
    return spark.sql("""
        SELECT 
            CORR(numberOfClicksHistory, timeOnPageHistory) as corr_clicks_time,
            CORR(numberOfClicksHistory, scrollPercentageHistory) as corr_clicks_scroll,
            CORR(timeOnPageHistory, scrollPercentageHistory) as corr_time_scroll
        FROM tab_treino
    """).toPandas()

# Inicializar Spark e carregar dados
try:
    spark = init_spark()
    #st.sidebar.success("‚úÖ Conex√£o com Spark estabelecida")
    print("Conex√£o com Spark estabelecida com sucesso")
except Exception as e:
    st.sidebar.error(f"‚ùå Erro ao conectar com Spark: {str(e)}")
    st.stop()

# Sidebar com navega√ß√£o mais limpa
st.sidebar.title("üìä Dashboard G1")

# Menu de navega√ß√£o simplificado
page = st.sidebar.radio(
    "Navega√ß√£o",
    options=["In√≠cio", "Vis√£o Geral", "Perfil dos Usu√°rios", "Cold Start", 
             "Rec√™ncia e Engajamento", "An√°lise de Conte√∫do", 
             "Distribui√ß√£o Temporal", "Conclus√£o"]
)

# Mover as informa√ß√µes de debug para uma se√ß√£o expans√≠vel
with st.sidebar.expander("‚ÑπÔ∏è Informa√ß√µes de Debug", expanded=False):
    st.write("Verificando caminhos:")
    st.write(f"Conte√∫do de /app/datalake/silver/:")
    st.write(os.listdir("/app/datalake/silver/"))
    st.write("‚úÖ Arquivo treino carregado")
    st.write(f"Registros treino: {spark.sql('SELECT COUNT(*) FROM tab_treino').collect()[0][0]:,}")
    st.write("‚úÖ Arquivo itens carregado")
    st.write(f"Registros itens: {spark.sql('SELECT COUNT(*) FROM tab_itens').collect()[0][0]:,}")
    st.write("‚úÖ Conex√£o com Spark estabelecida")

# Fun√ß√µes de visualiza√ß√£o usando dados cacheados
def show_objective(objective_text):
    """Exibe o objetivo da an√°lise para a guia atual."""
    st.markdown(f"### Objetivo\n{objective_text}")

def show_visao_geral():
    show_objective("Esta se√ß√£o fornece uma vis√£o geral das intera√ß√µes dos usu√°rios e do desempenho do conte√∫do.")
    st.title("üéØ Vis√£o Geral")
    
    metrics = get_basic_metrics()
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total de Usu√°rios", f"{metrics['total_users']:,}")
    with col2:
        st.metric("Total de Intera√ß√µes", f"{metrics['total_interactions']:,}")
    with col3:
        st.metric("M√©dia Intera√ß√µes/Usu√°rio", f"{metrics['avg_interactions']:,}")
    with col4:
        st.metric("Total de Not√≠cias", f"{metrics['total_news']:,}")

def show_perfil_usuarios(spark):
    show_objective("Analise os perfis dos usu√°rios para identificar padr√µes de comportamento e segmentar usu√°rios com base em intera√ß√µes.")
    st.title("üë• An√°lise do Perfil dos Usu√°rios")
    
    # Obter dados de distribui√ß√£o de usu√°rios
    user_dist = get_user_distribution()
    
    if user_dist.empty:
        st.warning("Nenhum dado dispon√≠vel para a distribui√ß√£o de usu√°rios.")
        return
    
    # Criar colunas para os gr√°ficos
    col1, col2 = st.columns(2)

    with col1:
        # Gr√°fico de Distribui√ß√£o de Usu√°rios
        fig1 = px.bar(
            user_dist,
            x='userType',
            y='unique_users',
            title='Distribui√ß√£o de Usu√°rios por Tipo',
            color_discrete_sequence=[THEME_COLORS['primary']]
        )
        fig1.update_layout(
            plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            font=dict(color=THEME_COLORS['text'])
        )
        st.plotly_chart(fig1, use_container_width=True)

    with col2:
        # Gr√°fico de M√©tricas de Engajamento
        engagement_metrics = get_engagement_metrics()
        fig2 = px.bar(
            engagement_metrics,
            x='userType',
            y=['media_clicks', 'media_tempo_minutos', 'media_scroll'],
            title='M√©tricas de Engajamento por Tipo de Usu√°rio',
            barmode='group',
            color_discrete_sequence=[THEME_COLORS['primary'], THEME_COLORS['secondary'], '#4CAF50']
        )
        fig2.update_layout(
            plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            font=dict(color=THEME_COLORS['text'])
        )
        st.plotly_chart(fig2, use_container_width=True)

    st.subheader("üìÖ Padr√µes de Acesso")
    hourly_pattern = get_hourly_pattern()
    fig3 = go.Figure()
    fig3.add_trace(go.Scatter(x=hourly_pattern['hora'], y=hourly_pattern['total_acessos'],
                            name='Total de Acessos', mode='lines'))
    fig3.add_trace(go.Scatter(x=hourly_pattern['hora'], y=hourly_pattern['usuarios_unicos'],
                            name='Usu√°rios √önicos', mode='lines'))
    fig3.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text']),     # Texto do gr√°fico
        title='Distribui√ß√£o de Acessos por Hora do Dia'
    )
    st.plotly_chart(fig3, use_container_width=True)

    st.markdown("""
    ### üë• Insights sobre os Usu√°rios
    - Diferentes perfis apresentam padr√µes distintos de consumo
    - Hor√°rios de pico bem definidos ao longo do dia
    - Varia√ß√£o significativa no tempo de leitura entre tipos de usu√°rio
    - Oportunidade para personaliza√ß√£o temporal das recomenda√ß√µes
    """)

def show_cold_start(spark):
    show_objective("Aborde o desafio do cold start desenvolvendo estrat√©gias para melhorar a experi√™ncia de novos usu√°rios com dados limitados.")
    st.title("üÜï An√°lise de Cold Start")
    
    cold_start = get_cold_start_analysis(spark)
    
    if cold_start.empty:
        st.warning("Nenhum dado dispon√≠vel para a an√°lise de cold start.")
        return
    
    fig = go.Figure()
    fig.add_trace(go.Pie(
        labels=cold_start['nivel_interacao'],
        values=cold_start['num_users'],
        marker=dict(colors=[THEME_COLORS['primary'], THEME_COLORS['secondary'], '#4CAF50', '#FF9800']),
        textinfo='percent+label'
    ))
    
    fig.update_layout(
        title='Distribui√ß√£o de Usu√°rios por N√≠vel de Intera√ß√£o',
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text'])      # Texto do gr√°fico
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("""
    ### üîç Desafios Identificados
    - Alto percentual de usu√°rios com poucas intera√ß√µes
    - Necessidade de estrat√©gias para novos usu√°rios
    - Import√¢ncia do primeiro contato
    
    ### ‚ö° Estrat√©gias Sugeridas
    - Usar popularidade global para novos usu√°rios
    - Implementar recomenda√ß√µes baseadas em contexto
    - Coletar informa√ß√µes m√≠nimas no cadastro
    """)

def show_recencia_engajamento():
    show_objective("Explore m√©tricas de rec√™ncia e engajamento para entender a atividade dos usu√°rios ao longo do tempo.")
    st.title("An√°lise de Rec√™ncia e Engajamento")
    recency = get_recency_analysis()
    
    fig = go.Figure()
    fig.add_trace(go.Bar(x=recency['periodo'], y=recency['num_users'],
                        marker=dict(color=THEME_COLORS['primary'])))
    fig.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text']),     # Texto do gr√°fico
        title='Distribui√ß√£o de Usu√°rios por Per√≠odo de Atividade'
    )
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("""
    ### ‚è∞ Padr√µes Temporais
    - Diferentes n√≠veis de atividade ao longo do tempo
    - Import√¢ncia da rec√™ncia nas intera√ß√µes
    - Ciclos de engajamento identificados
    
    ### üéØ Recomenda√ß√µes
    - Priorizar conte√∫do recente
    - Reativar usu√°rios inativos
    - Balancear novidade e relev√¢ncia
    """)

def show_analise_conteudo():
    show_objective("Analise o desempenho do conte√∫do para avaliar quais tipos de conte√∫do geram mais engajamento.")
    st.title("An√°lise de Conte√∫do")
    
    top_cats = get_top_categories()
    
    # Gr√°fico de Top Categorias mais Lidas
    fig1 = px.bar(top_cats, x='category', y=['total_reads', 'unique_readers'],
                  title='Top 10 Categorias mais Lidas',
                  barmode='group')
    fig1.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text'])
    )
    st.plotly_chart(fig1, use_container_width=True)
    
    # Gr√°fico de M√©tricas de Engajamento por Categoria
    fig2 = px.bar(top_cats, x='category', y=['avg_time_minutes', 'avg_scroll'],
                  title='M√©tricas de Engajamento por Categoria',
                  barmode='group')
    fig2.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text'])
    )
    st.plotly_chart(fig2, use_container_width=True)

    st.subheader("üîÑ Correla√ß√£o entre M√©tricas de Engajamento")
    correlation = get_correlation_metrics()
    
    st.write("Correla√ß√µes:")
    st.write("- Clicks vs Tempo: ", round(correlation['corr_clicks_time'][0], 2))
    st.write("- Clicks vs Scroll: ", round(correlation['corr_clicks_scroll'][0], 2))
    st.write("- Tempo vs Scroll: ", round(correlation['corr_time_scroll'][0], 2))

    st.markdown("""
    ### üì∞ Insights sobre Conte√∫do
    - Categorias populares t√™m padr√µes distintos de engajamento
    - Tempo de leitura varia significativamente entre categorias
    - Correla√ß√£o interessante entre m√©tricas de engajamento
    - Oportunidade para recomenda√ß√µes baseadas em padr√µes de consumo
    """)

# Fun√ß√£o para mostrar a distribui√ß√£o temporal das intera√ß√µes
def show_temporal_distribution(spark):
    show_objective("Descubra padr√µes temporais e sazonalidades nas intera√ß√µes dos usu√°rios ao longo do tempo.")
    st.title("üìà Distribui√ß√£o Temporal das Intera√ß√µes")
    
    temporal_dist = get_temporal_distribution(spark)
    
    if temporal_dist.empty:
        st.warning("Nenhum dado dispon√≠vel para a distribui√ß√£o temporal.")
        return
    
    # Gr√°fico com dados v√°lidos
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=temporal_dist['data'],
        y=temporal_dist['total_interacoes'],
        name='Total de Intera√ß√µes',
        line=dict(color=THEME_COLORS['primary'], width=2)
    ))
    fig.add_trace(go.Scatter(
        x=temporal_dist['data'],
        y=temporal_dist['usuarios_unicos'],
        name='Usu√°rios √önicos',
        line=dict(color=THEME_COLORS['secondary'], width=2)
    ))
    
    fig.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text']),     # Texto do gr√°fico
        title='Distribui√ß√£o Temporal das Intera√ß√µes'
    )
    
    st.subheader("üìà Distribui√ß√£o Temporal das Intera√ß√µes")
    temporal_dist = get_temporal_distribution(spark)
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=temporal_dist['data'], y=temporal_dist['total_interacoes'],
                            name='Total de Intera√ß√µes', mode='lines'))
    fig.add_trace(go.Scatter(x=temporal_dist['data'], y=temporal_dist['usuarios_unicos'],
                            name='Usu√°rios √önicos', mode='lines'))
    fig.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text']),     # Texto do gr√°fico
        title='Distribui√ß√£o Temporal das Intera√ß√µes'
    )
    st.plotly_chart(fig, use_container_width=True)

    st.markdown("""
    ### üìä Insights Principais
    - Volume significativo de dados com mais de 8 milh√µes de intera√ß√µes
    - Base diversificada de usu√°rios com diferentes padr√µes de consumo
    - Padr√µes temporais claros nas intera√ß√µes
    - Oportunidade para personaliza√ß√£o em escala
    """)

def show_home():
    
    st.title("üìä Dashboard G1 - Sistema de Recomenda√ß√£o")
    
    st.markdown("""
    Bem-vindo ao Dashboard G1, uma plataforma interativa para an√°lise explorat√≥ria de dados do nosso sistema de recomenda√ß√£o. 
    Este dashboard foi desenvolvido para fornecer insights valiosos sobre o comportamento dos usu√°rios e o desempenho do conte√∫do.

    ### Objetivos da An√°lise:
    - **Entender o Perfil dos Usu√°rios**: Identificar padr√µes de comportamento e segmentar usu√°rios com base em suas intera√ß√µes.
    - **Analisar o Desempenho do Conte√∫do**: Avaliar quais tipos de conte√∫do geram mais engajamento e como os usu√°rios interagem com eles.
    - **Explorar Padr√µes Temporais**: Descobrir tend√™ncias e sazonalidades nas intera√ß√µes dos usu√°rios ao longo do tempo.
    - **Abordar o Desafio do Cold Start**: Desenvolver estrat√©gias para melhorar a experi√™ncia de novos usu√°rios com base em dados limitados.

    Navegue pelas diferentes se√ß√µes para explorar os dados e descobrir insights que podem ajudar a otimizar nosso sistema de recomenda√ß√£o.
    """)

# Adicione a fun√ß√£o de conclus√£o
def show_conclusion():
    st.title("üîç Conclus√£o da An√°lise")
    
    st.markdown("""
    Ap√≥s uma an√°lise detalhada dos dados, chegamos √†s seguintes conclus√µes:

    - **Diversidade de Usu√°rios**: Identificamos uma base diversificada de usu√°rios com diferentes padr√µes de consumo, o que sugere a necessidade de personaliza√ß√£o.
    - **Conte√∫do Popular**: Certas categorias de conte√∫do geram mais engajamento, indicando √°reas de foco para futuras recomenda√ß√µes.
    - **Padr√µes Temporais**: Observamos padr√µes claros de intera√ß√£o ao longo do tempo, o que pode ser utilizado para otimizar o timing das recomenda√ß√µes.
    - **Desafios do Cold Start**: Estrat√©gias espec√≠ficas s√£o necess√°rias para engajar novos usu√°rios e melhorar sua experi√™ncia inicial.

    Continuaremos a monitorar e ajustar nosso sistema de recomenda√ß√£o com base nesses insights para oferecer uma experi√™ncia cada vez mais personalizada e eficaz.
    """)

# Execu√ß√£o da p√°gina selecionada
if page == "In√≠cio":
    show_home()
elif page == "Vis√£o Geral":
    show_visao_geral()
elif page == "Perfil dos Usu√°rios":
    show_perfil_usuarios(spark)
elif page == "Cold Start":
    show_cold_start(spark)
elif page == "Rec√™ncia e Engajamento":
    show_recencia_engajamento()
elif page == "An√°lise de Conte√∫do":
    show_analise_conteudo()
elif page == "Distribui√ß√£o Temporal":
    show_temporal_distribution(spark)
elif page == "Conclus√£o":
    show_conclusion() 