import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pyspark.sql import SparkSession
import os
import logging
import concurrent.futures
import time
from typing import Dict, Any

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configura√ß√£o de tema e cores
THEME_COLORS = {
    'primary': '#2196F3',
    'secondary': '#FF9800',
    'background': '#000000',
    'surface': '#1E1E1E',   
    'text': '#FFFFFF'       
}

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Dashboard G1 - Sistema de Recomenda√ß√£o",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado
st.markdown("""
    <style>
    .main {
        background-color: #000000;  /* Fundo preto */
        color: #FFFFFF;              /* Texto branco */
    }
    .stMetric {
        background-color: #1E1E1E;   /* Cor de superf√≠cie */
        color: #FFFFFF;              /* Texto branco */
    }
    </style>
""", unsafe_allow_html=True)

# Inicializar Spark e carregar dados
@st.cache_resource
def init_spark():
    spark = SparkSession.builder \
        .appName("G1 Recommendations") \
        .config("spark.sql.parquet.datetimeRebaseModeInRead", "CORRECTED") \
        .config("spark.sql.parquet.datetimeRebaseModeInWrite", "CORRECTED") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.sql.shuffle.partitions", "10") \
        .config("spark.sql.autoBroadcastJoinThreshold", "10m") \
        .config("spark.memory.fraction", "0.8") \
        .config("spark.memory.storageFraction", "0.3") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.sql.inMemoryColumnarStorage.compressed", "true") \
        .config("spark.sql.inMemoryColumnarStorage.batchSize", "10000") \
        .config("spark.sql.shuffle.partitions", "8") \
        .getOrCreate()
    
    # Configura√ß√µes adicionais ap√≥s a cria√ß√£o da sess√£o
    spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
    spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", "10000")
    spark.conf.set("spark.sql.shuffle.partitions", "8")
    spark.conf.set("spark.default.parallelism", "8")
    
    # Ler os arquivos parquet com particionamento
    treino = spark.read \
        .option("mergeSchema", "false") \
        .parquet("/app/datalake/silver/treino") \
    
    itens = spark.read \
    .option("mergeSchema", "false") \
    .parquet("datalake/silver/itens") \
    
    # Registrar as tabelas tempor√°rias
    treino.createOrReplaceTempView("tab_treino")
    itens.createOrReplaceTempView("tab_itens")
    
    return spark

# Cache para queries frequentes
@st.cache_data
def get_basic_metrics():
    return {
        "total_users": spark.sql('SELECT COUNT(DISTINCT userId) FROM tab_treino').collect()[0][0],
        "total_interactions": spark.sql('SELECT COUNT(*) FROM tab_treino').collect()[0][0],
        "avg_interactions": spark.sql('SELECT CAST(COUNT(*)/COUNT(DISTINCT userId) AS INT) FROM tab_treino').collect()[0][0],
        "total_news": spark.sql('SELECT COUNT(DISTINCT history) FROM tab_treino').collect()[0][0]
    }

@st.cache_data
def get_user_distribution():
    return spark.sql("""
        SELECT 
            userType,
            COUNT(DISTINCT userId) as unique_users,
            CAST(AVG(numberOfClicksHistory) as INT) as avg_clicks,
            CAST(AVG(timeOnPageHistory) as INT) as avg_time,
            CAST(AVG(scrollPercentageHistory) as INT) as avg_scroll
        FROM tab_treino
        GROUP BY userType
        ORDER BY unique_users DESC
    """).toPandas()

@st.cache_data
def get_cold_start_analysis():
    return spark.sql("""
        WITH user_interactions AS (
            SELECT userId, COUNT(*) as interaction_count
            FROM tab_treino
            GROUP BY userId
        )
        SELECT 
            CASE 
                WHEN interaction_count < 5 THEN 'Muito Baixo (< 5)'
                WHEN interaction_count < 10 THEN 'Baixo (5-9)'
                WHEN interaction_count < 20 THEN 'M√©dio (10-19)'
                ELSE 'Alto (20+)'
            END as nivel_interacao,
            COUNT(*) as num_users,
            CAST((COUNT(*) * 100.0 / SUM(COUNT(*)) OVER ()) AS DECIMAL(10,2)) as percentual
        FROM user_interactions
        GROUP BY 
            CASE 
                WHEN interaction_count < 5 THEN 'Muito Baixo (< 5)'
                WHEN interaction_count < 10 THEN 'Baixo (5-9)'
                WHEN interaction_count < 20 THEN 'M√©dio (10-19)'
                ELSE 'Alto (20+)'
            END
        ORDER BY num_users DESC
    """).toPandas()

@st.cache_data
def get_recency_analysis():
    return spark.sql("""
        WITH user_recency AS (
            SELECT 
                userId,
                DATEDIFF(MAX(timestampHistory), MIN(timestampHistory)) as days_active
            FROM tab_treino
            GROUP BY userId
        )
        SELECT 
            CASE 
                WHEN days_active <= 1 THEN '1 dia'
                WHEN days_active <= 7 THEN '1 semana'
                WHEN days_active <= 30 THEN '1 m√™s'
                ELSE 'Mais de 1 m√™s'
            END as periodo,
            COUNT(*) as num_users
        FROM user_recency
        GROUP BY 
            CASE 
                WHEN days_active <= 1 THEN '1 dia'
                WHEN days_active <= 7 THEN '1 semana'
                WHEN days_active <= 30 THEN '1 m√™s'
                ELSE 'Mais de 1 m√™s'
            END
        ORDER BY num_users DESC
    """).toPandas()

@st.cache_data
def get_content_analysis():
    return spark.sql("""
        SELECT 
            i.page as category,
            COUNT(*) as total_reads,
            COUNT(DISTINCT t.userId) as unique_readers,
            CAST(AVG(t.timeOnPageHistory) as INT) as avg_time
        FROM tab_treino t
        JOIN tab_itens i ON t.history = i.page
        GROUP BY i.page
        HAVING COUNT(*) > 100
        ORDER BY total_reads DESC
        LIMIT 10
    """).toPandas()

@st.cache_data
def get_temporal_distribution():
    return spark.sql("""
        SELECT 
            DAYOFWEEK(timestampHistory) as dia_semana,
            HOUR(timestampHistory) as hora,
            COUNT(DISTINCT userId) as num_usuarios,
            i.page as categoria
        FROM tab_treino t
        JOIN tab_itens i ON t.history = i.page
        GROUP BY 
            DAYOFWEEK(timestampHistory),
            HOUR(timestampHistory),
            i.page
        ORDER BY dia_semana, hora
    """).toPandas()

@st.cache_data
def get_engagement_metrics():
    return spark.sql("""
        SELECT 
            userType,
            CAST(AVG(numberOfClicksHistory) as INT) as media_clicks,
            CAST(AVG(timeOnPageHistory)/60 as INT) as media_tempo_minutos,
            CAST(AVG(scrollPercentageHistory) as INT) as media_scroll
        FROM tab_treino
        GROUP BY userType
        ORDER BY media_tempo_minutos DESC
    """).toPandas()

@st.cache_data
def get_hourly_pattern():
    return spark.sql("""
        SELECT 
            HOUR(timestampHistory) as hora,
            COUNT(*) as total_acessos,
            COUNT(DISTINCT userId) as usuarios_unicos
        FROM tab_treino
        GROUP BY HOUR(timestampHistory)
        ORDER BY hora
    """).toPandas()

@st.cache_data
def get_top_categories():
    return spark.sql("""
        SELECT 
            i.page as category,
            COUNT(*) as total_reads,
            COUNT(DISTINCT t.userId) as unique_readers,
            CAST(AVG(t.timeOnPageHistory)/60 as INT) as avg_time_minutes,
            CAST(AVG(t.scrollPercentageHistory) as INT) as avg_scroll
        FROM tab_treino t
        JOIN tab_itens i ON t.history = i.page
        GROUP BY i.page
        HAVING COUNT(*) > 100
        ORDER BY total_reads DESC
        LIMIT 10
    """).toPandas()

@st.cache_data
def get_correlation_metrics():
    return spark.sql("""
        SELECT 
            CORR(numberOfClicksHistory, timeOnPageHistory) as corr_clicks_time,
            CORR(numberOfClicksHistory, scrollPercentageHistory) as corr_clicks_scroll,
            CORR(timeOnPageHistory, scrollPercentageHistory) as corr_time_scroll
        FROM tab_treino
    """).toPandas()

# @st.cache_data
# def get_content_similarity():
#     return spark.sql("""
#         WITH filtered_articles AS (
#             -- Pr√©-filtrar artigos com n√∫mero m√≠nimo de leitores
#             SELECT history, COUNT(DISTINCT userId) as reader_count
#             FROM tab_treino
#             GROUP BY history
#             HAVING COUNT(DISTINCT userId) >= 10
#         ),
#         sampled_interactions AS (
#             -- Amostrar apenas uma parte dos dados para an√°lise
#             SELECT t.userId, t.history
#             FROM tab_treino t
#             JOIN filtered_articles fa ON t.history = fa.history
#             WHERE userId IN (
#                 SELECT DISTINCT userId 
#                 FROM tab_treino 
#                 GROUP BY userId 
#                 HAVING COUNT(*) >= 5
#                 LIMIT 10000
#             )
#         ),
#         article_pairs AS (
#             -- Calcular co-ocorr√™ncias com dados amostrados
#             SELECT 
#                 a.history as article1,
#                 b.history as article2,
#                 COUNT(DISTINCT a.userId) as co_occurrences
#             FROM sampled_interactions a
#             JOIN sampled_interactions b 
#                 ON a.userId = b.userId 
#                 AND a.history < b.history
#             GROUP BY a.history, b.history
#             HAVING COUNT(DISTINCT a.userId) >= 3
#         )
#         SELECT 
#             ap.article1,
#             ap.article2,
#             ap.co_occurrences,
#             fa1.reader_count as readers_article1,
#             fa2.reader_count as readers_article2,
#             CAST(ap.co_occurrences / SQRT(fa1.reader_count * fa2.reader_count) AS DOUBLE) as similarity_score
#         FROM article_pairs ap
#         JOIN filtered_articles fa1 ON ap.article1 = fa1.history
#         JOIN filtered_articles fa2 ON ap.article2 = fa2.history
#         ORDER BY similarity_score DESC
#         LIMIT 20
#     """).toPandas()

@st.cache_data
def get_reading_sequence():
    return spark.sql("""
        WITH ordered_reads AS (
            SELECT 
                userId,
                history,
                timestampHistory,
                LAG(history) OVER (PARTITION BY userId ORDER BY timestampHistory) as prev_article
            FROM tab_treino
        )
        SELECT 
            prev_article,
            history as next_article,
            COUNT(*) as frequency
        FROM ordered_reads
        WHERE prev_article IS NOT NULL
        GROUP BY prev_article, history
        HAVING COUNT(*) > 10
        ORDER BY frequency DESC
        LIMIT 15
    """).toPandas()

@st.cache_data
def get_seasonality_analysis():
    return spark.sql("""
        SELECT 
            DAYOFWEEK(timestampHistory) as dia_semana,
            HOUR(timestampHistory) as hora,
            COUNT(*) as total_leituras,
            COUNT(DISTINCT userId) as usuarios_unicos
        FROM tab_treino
        GROUP BY DAYOFWEEK(timestampHistory), HOUR(timestampHistory)
        ORDER BY dia_semana, hora
    """).toPandas()

def load_data_sequential() -> Dict[str, Any]:
    data_cache = {}
    
    # Placeholder para mostrar progresso
    progress_placeholder = st.empty()
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Lista de todas as fun√ß√µes de carregamento de dados em ordem de prioridade
    data_loaders = [
        ('basic_metrics', lambda: get_basic_metrics(), 'M√©tricas B√°sicas'),
        ('user_distribution', lambda: get_user_distribution(), 'Distribui√ß√£o de Usu√°rios'),
        ('cold_start', lambda: get_cold_start_analysis(), 'An√°lise de Cold Start'),
        ('recency', lambda: get_recency_analysis(), 'An√°lise de Rec√™ncia'),
        ('content_analysis', lambda: get_content_analysis(), 'An√°lise de Conte√∫do'),
        ('temporal_dist', lambda: get_temporal_distribution(), 'Distribui√ß√£o Temporal'),
        ('engagement', lambda: get_engagement_metrics(), 'M√©tricas de Engajamento'),
        ('hourly_pattern', lambda: get_hourly_pattern(), 'Padr√µes por Hora'),
        ('top_categories', lambda: get_top_categories(), 'Top Categorias'),
        ('correlation', lambda: get_correlation_metrics(), 'Correla√ß√µes'),
        ('seasonality', lambda: get_seasonality_analysis(), 'Sazonalidade'),
        ('reading_sequence', lambda: get_reading_sequence(), 'Sequ√™ncias de Leitura')
    ]
    
    total_queries = len(data_loaders)
    
    for idx, (name, func, description) in enumerate(data_loaders, 1):
        try:
            # Atualizar status
            progress = idx / total_queries
            progress_bar.progress(progress)
            status_text.text(f"Carregando {description}... ({idx}/{total_queries})")
            
            # Executar query
            result = func()
            data_cache[name] = result
            
        except Exception as e:
            st.error(f"Erro ao carregar {description}: {str(e)}")
    
    # Limpar elementos de progresso
    progress_placeholder.empty()
    progress_bar.empty()
    status_text.empty()
    
    return data_cache

def main():
    # Configura√ß√£o inicial do Spark
    try:
        global spark, data_cache  # Tornar data_cache global tamb√©m
        spark = init_spark()
        print("Conex√£o com Spark estabelecida com sucesso")
    except Exception as e:
        st.error(f"‚ùå Erro ao conectar com Spark: {str(e)}")
        st.stop()
    
    # Configurar o layout principal
    st.sidebar.title("Analise Exploratoria de Dados")
    
    # Menu de navega√ß√£o
    page = st.sidebar.radio(
        "Navega√ß√£o",
        options=["In√≠cio", "Vis√£o Geral", "Perfil dos Usu√°rios", "Cold Start", 
                "Rec√™ncia e Engajamento", "An√°lise de Conte√∫do", 
                "Distribui√ß√£o Temporal", "Padr√µes Avan√ßados", "Conclus√£o"]
    )
    
    # Carregar dados sequencialmente com feedback visual
    with st.spinner('Inicializando an√°lise de dados...'):
        data_cache = load_data_sequential()
        
        # Verificar dados essenciais
        required_data = ['basic_metrics', 'user_distribution', 'cold_start']
        missing_data = [key for key in required_data if key not in data_cache]
        
        if missing_data:
            st.error(f"Falha ao carregar dados essenciais: {', '.join(missing_data)}")
            st.stop()

    # T√≠tulo principal apenas na p√°gina inicial
    if page == "In√≠cio":
        st.title("üìä Dashboard G1 - Sistema de Recomenda√ß√£o")
    
    # Execu√ß√£o da p√°gina selecionada
    if page == "In√≠cio":
        show_home()
    elif page == "Vis√£o Geral":
        show_visao_geral(data_cache)
    elif page == "Perfil dos Usu√°rios":
        show_perfil_usuarios(data_cache)
    elif page == "Cold Start":
        show_cold_start(data_cache)
    elif page == "Rec√™ncia e Engajamento":
        show_recencia_engajamento(data_cache)
    elif page == "An√°lise de Conte√∫do":
        show_analise_conteudo(data_cache)
    elif page == "Distribui√ß√£o Temporal":
        show_temporal_distribution(data_cache)
    elif page == "Conclus√£o":
        show_conclusion()
    elif page == "Padr√µes Avan√ßados":
        show_advanced_patterns(data_cache)

def show_objective(text):
    """Exibe o objetivo da se√ß√£o atual."""
    st.markdown(f"#### Objetivo\n{text}")

def show_visao_geral(data_cache):
    """Mostra a vis√£o geral do sistema de recomenda√ß√£o."""
    metrics = data_cache.get('basic_metrics')
    if metrics is None:
        st.error("Dados b√°sicos n√£o dispon√≠veis")
        return
    
    st.subheader("Vis√£o Geral")
    
    show_objective("""
    Fornecer uma vis√£o abrangente do sistema de recomenda√ß√£o, apresentando m√©tricas-chave 
    de engajamento, distribui√ß√£o de usu√°rios e principais indicadores de desempenho.
    """)
    
    # M√©tricas principais
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total de Usu√°rios", f"{metrics['total_users']:,}")
    with col2:
        st.metric("Total de Intera√ß√µes", f"{metrics['total_interactions']:,}")
    with col3:
        st.metric("M√©dia Intera√ß√µes/Usu√°rio", f"{metrics['avg_interactions']:,}")
    with col4:
        st.metric("Total de Not√≠cias", f"{metrics['total_news']:,}")
    
    # Visualiza√ß√µes
    st.subheader("üìä Distribui√ß√£o de Intera√ß√µes")
    
    user_dist = data_cache.get('user_distribution')
    if user_dist is not None and not user_dist.empty:
        fig = px.bar(
            user_dist,
            x='userType',
            y='unique_users',
            title='Distribui√ß√£o de Usu√°rios por Tipo',
            color_discrete_sequence=[THEME_COLORS['primary']]
        )
        fig.update_layout(
            plot_bgcolor=THEME_COLORS['background'],
            paper_bgcolor=THEME_COLORS['background'],
            font=dict(color=THEME_COLORS['text'])
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Novos insights mais detalhados
    st.markdown(f"""
    ### üìà Insights Principais
    - **Volume de Dados**: Base com {metrics['total_users']:,} usu√°rios ativos e {metrics['total_interactions']:,} intera√ß√µes
    - **Engajamento**: M√©dia de {metrics['avg_interactions']:,} intera√ß√µes por usu√°rio
    - **Diversidade de Conte√∫do**: {metrics['total_news']:,} not√≠cias diferentes consumidas
    - **Oportunidades**:
        - Personaliza√ß√£o baseada no hist√≥rico de intera√ß√µes
        - Segmenta√ß√£o por padr√µes de consumo
        - Otimiza√ß√£o da distribui√ß√£o de conte√∫do
    """)

def show_perfil_usuarios(data_cache):
    st.subheader("üë• An√°lise do Perfil dos Usu√°rios")
    
    show_objective("""
    Compreender os diferentes perfis de usu√°rios, seus padr√µes de comportamento e prefer√™ncias, 
    visando melhorar a segmenta√ß√£o e personaliza√ß√£o das recomenda√ß√µes.
    """)
    
    user_dist = data_cache.get('user_distribution')
    if user_dist is None or user_dist.empty:
        st.warning("Nenhum dado dispon√≠vel para a distribui√ß√£o de usu√°rios.")
        return
    
    # Criar colunas para os gr√°ficos
    col1, col2 = st.columns(2)

    with col1:
        # Gr√°fico de Distribui√ß√£o de Usu√°rios
        fig1 = px.bar(
            user_dist,
            x='userType',
            y='unique_users',
            title='Distribui√ß√£o de Usu√°rios por Tipo',
            color_discrete_sequence=[THEME_COLORS['primary']]
        )
        fig1.update_layout(
            plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            font=dict(color=THEME_COLORS['text'])
        )
        st.plotly_chart(fig1, use_container_width=True)

    with col2:
        # Gr√°fico de M√©tricas de Engajamento
        engagement_metrics = data_cache['engagement']
        fig2 = px.bar(
            engagement_metrics,
            x='userType',
            y=['media_clicks', 'media_tempo_minutos', 'media_scroll'],
            title='M√©tricas de Engajamento por Tipo de Usu√°rio',
            barmode='group',
            color_discrete_sequence=[THEME_COLORS['primary'], THEME_COLORS['secondary'], '#4CAF50']
        )
        fig2.update_layout(
            plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
            font=dict(color=THEME_COLORS['text'])
        )
        st.plotly_chart(fig2, use_container_width=True)

    st.subheader("üìÖ Padr√µes de Acesso")
    hourly_pattern = data_cache['hourly_pattern']
    fig3 = go.Figure()
    fig3.add_trace(go.Scatter(x=hourly_pattern['hora'], y=hourly_pattern['total_acessos'],
                            name='Total de Acessos', mode='lines'))
    fig3.add_trace(go.Scatter(x=hourly_pattern['hora'], y=hourly_pattern['usuarios_unicos'],
                            name='Usu√°rios √önicos', mode='lines'))
    fig3.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text']),     # Texto do gr√°fico
        title='Distribui√ß√£o de Acessos por Hora do Dia'
    )
    st.plotly_chart(fig3, use_container_width=True)

    # Novos insights mais estruturados
    st.markdown("""
    ### üë• Insights sobre os Usu√°rios
    - **Segmenta√ß√£o de Perfis**:
        - Identificados padr√µes distintos de consumo por tipo de usu√°rio
        - Varia√ß√£o significativa no tempo m√©dio de leitura
        - Diferentes n√≠veis de engajamento por segmento

    - **Comportamento Temporal**:
        - Picos de acesso em hor√°rios comerciais
        - Padr√µes distintos entre dias √∫teis e finais de semana
        - Oportunidades de engajamento em hor√°rios espec√≠ficos

    - **M√©tricas de Engajamento**:
        - Correla√ß√£o entre tempo de leitura e scroll
        - Diferentes padr√µes de navega√ß√£o por perfil
        - Identifica√ß√£o de usu√°rios mais engajados

    - **Recomenda√ß√µes**:
        - Personaliza√ß√£o por segmento de usu√°rio
        - Adapta√ß√£o do conte√∫do ao hor√°rio de acesso
        - Estrat√©gias espec√≠ficas por perfil de engajamento
    """)

def show_cold_start(data_cache):
    st.subheader("üÜï An√°lise de Cold Start")
    
    show_objective("""
    Analisar o desafio de novos usu√°rios e usu√°rios com poucas intera√ß√µes, buscando 
    estrat√©gias efetivas para melhorar a experi√™ncia inicial e aumentar o engajamento.
    """)
    
    cold_start = data_cache['cold_start']
    
    if cold_start.empty:
        st.warning("Nenhum dado dispon√≠vel para a an√°lise de cold start.")
        return
    
    fig = go.Figure()
    fig.add_trace(go.Pie(
        labels=cold_start['nivel_interacao'],
        values=cold_start['num_users'],
        marker=dict(colors=[THEME_COLORS['primary'], THEME_COLORS['secondary'], '#4CAF50', '#FF9800']),
        textinfo='percent+label'
    ))
    
    fig.update_layout(
        title='Distribui√ß√£o de Usu√°rios por N√≠vel de Intera√ß√£o',
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text'])      # Texto do gr√°fico
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Calcular percentuais para insights
    total_users = cold_start['num_users'].sum()
    low_interactions = cold_start[cold_start['nivel_interacao'] == 'Muito Baixo (< 5)']['num_users'].iloc[0]
    high_interactions = cold_start[cold_start['nivel_interacao'] == 'Alto (20+)']['num_users'].iloc[0]
    
    low_percent = (low_interactions / total_users) * 100
    high_percent = (high_interactions / total_users) * 100
    
    st.markdown(f"""
    ### üîç An√°lise do Cold Start
    - **Distribui√ß√£o de Intera√ß√µes**:
        - {low_percent:.1f}% dos usu√°rios t√™m menos de 5 intera√ß√µes
        - {high_percent:.1f}% s√£o usu√°rios altamente ativos (20+ intera√ß√µes)
        - Desafio cr√≠tico com novos usu√°rios

    - **Desafios Identificados**:
        - Baixa reten√ß√£o inicial de novos usu√°rios
        - Limita√ß√£o de dados para personaliza√ß√£o
        - Necessidade de engajamento r√°pido

    - **Estrat√©gias Propostas**:
        1. **Recomenda√ß√µes Iniciais**:
            - Conte√∫do mais popular da plataforma
            - Tend√™ncias atuais e trending topics
            - Mix de categorias para descoberta de interesses

        2. **Coleta de Informa√ß√µes**:
            - Interesses b√°sicos no cadastro
            - Prefer√™ncias de categorias
            - Hor√°rios preferenciais de leitura

        3. **Engajamento Progressivo**:
            - Feedback r√°pido sobre recomenda√ß√µes
            - Gamifica√ß√£o das primeiras intera√ß√µes
            - Personaliza√ß√£o gradual do conte√∫do
    """)

def show_recencia_engajamento(data_cache):
    st.subheader("üìä An√°lise de Rec√™ncia e Engajamento")
    
    show_objective("""
    Avaliar os padr√µes de rec√™ncia nas intera√ß√µes dos usu√°rios e seus n√≠veis de engajamento, 
    identificando oportunidades para reten√ß√£o e reativa√ß√£o de usu√°rios.
    """)
    
    recency = data_cache['recency']
    
    fig = go.Figure()
    fig.add_trace(go.Bar(x=recency['periodo'], y=recency['num_users'],
                        marker=dict(color=THEME_COLORS['primary'])))
    fig.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text']),     # Texto do gr√°fico
        title='Distribui√ß√£o de Usu√°rios por Per√≠odo de Atividade'
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # Calcular m√©tricas para insights
    total_users = recency['num_users'].sum()
    active_week = recency[recency['periodo'] == '1 semana']['num_users'].iloc[0]
    week_percent = (active_week / total_users) * 100
    
    st.markdown(f"""
    ### ‚è∞ An√°lise de Rec√™ncia e Engajamento

    - **Padr√µes de Atividade**:
        - {week_percent:.1f}% dos usu√°rios ativos na √∫ltima semana
        - Forte correla√ß√£o entre rec√™ncia e engajamento
        - Ciclos claros de engajamento identificados

    - **Comportamento Temporal**:
        - Picos de atividade em hor√°rios espec√≠ficos
        - Padr√µes semanais de engajamento
        - Sazonalidade no consumo de conte√∫do

    - **M√©tricas de Reten√ß√£o**:
        - Taxa de retorno por segmento
        - Tempo m√©dio entre intera√ß√µes
        - Durabilidade do engajamento

    - **Estrat√©gias Recomendadas**:
        1. **Conte√∫do**:
            - Prioriza√ß√£o de not√≠cias recentes
            - Mix entre trending e personalizado
            - Adapta√ß√£o ao contexto temporal

        2. **Reten√ß√£o**:
            - Notifica√ß√µes personalizadas
            - Reengajamento de inativos
            - Campanhas baseadas em rec√™ncia

        3. **Otimiza√ß√£o**:
            - Timing das recomenda√ß√µes
            - Balanceamento de conte√∫do
            - Personaliza√ß√£o por padr√£o de uso
    """)

def show_analise_conteudo(data_cache):
    st.subheader("üì∞ An√°lise de Conte√∫do")
    
    show_objective("""
    Examinar o desempenho e impacto de diferentes tipos de conte√∫do, identificando 
    padr√µes de consumo e prefer√™ncias para otimizar as recomenda√ß√µes.
    """)
    
    top_cats = data_cache['top_categories']
    
    # Gr√°fico de Top Categorias mais Lidas
    fig1 = px.bar(top_cats, x='category', y=['total_reads', 'unique_readers'],
                  title='Top 10 Categorias mais Lidas',
                  barmode='group')
    fig1.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text'])
    )
    st.plotly_chart(fig1, use_container_width=True)
    
    # Gr√°fico de M√©tricas de Engajamento por Categoria
    fig2 = px.bar(top_cats, x='category', y=['avg_time_minutes', 'avg_scroll'],
                  title='M√©tricas de Engajamento por Categoria',
                  barmode='group')
    fig2.update_layout(
        plot_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        paper_bgcolor=THEME_COLORS['background'],  # Fundo do gr√°fico
        font=dict(color=THEME_COLORS['text'])
    )
    st.plotly_chart(fig2, use_container_width=True)

    st.subheader("üîÑ Correla√ß√£o entre M√©tricas de Engajamento")
    correlation = data_cache['correlation']
    
    st.write("Correla√ß√µes:")
    st.write("- Clicks vs Tempo: ", round(correlation['corr_clicks_time'][0], 2))
    st.write("- Clicks vs Scroll: ", round(correlation['corr_clicks_scroll'][0], 2))
    st.write("- Tempo vs Scroll: ", round(correlation['corr_time_scroll'][0], 2))

    st.markdown("""
    ### üì∞ Insights sobre Conte√∫do
    - Categorias populares t√™m padr√µes distintos de engajamento
    - Tempo de leitura varia significativamente entre categorias
    - Correla√ß√£o interessante entre m√©tricas de engajamento
    - Oportunidade para recomenda√ß√µes baseadas em padr√µes de consumo
    """)

def show_temporal_distribution(data_cache):
    st.subheader("üìà Distribui√ß√£o Temporal das Intera√ß√µes")
    
    show_objective("""
    Analisar os padr√µes de consumo de conte√∫do ao longo da semana e hor√°rios do dia, 
    identificando correla√ß√µes entre categorias espec√≠ficas e momentos de maior engajamento.
    """)
    
    temporal_dist = data_cache['temporal_dist']
    
    if temporal_dist.empty:
        st.warning("Nenhum dado dispon√≠vel para a distribui√ß√£o temporal.")
        return
    
    # Calcular total de usu√°rios por categoria
    categoria_counts = temporal_dist.groupby('categoria')['num_usuarios'].sum().sort_values(ascending=False)
    
    # Criar seletor de categoria com op√ß√£o "Todas" e ordenado por n√∫mero de usu√°rios
    categorias = ["Todas"] + list(categoria_counts.index)
    categoria_selecionada = st.selectbox(
        "Selecione a categoria de conte√∫do:",
        options=categorias,
        index=0  # Come√ßa com "Todas" selecionado
    )
    
    # Filtrar dados pela categoria selecionada (ou n√£o)
    if categoria_selecionada == "Todas":
        df_filtered = temporal_dist
        titulo = 'Distribui√ß√£o de Usu√°rios por Dia e Hora: Todas as Categorias'
    else:
        df_filtered = temporal_dist[temporal_dist['categoria'] == categoria_selecionada]
        titulo = f'Distribui√ß√£o de Usu√°rios por Dia e Hora: {categoria_selecionada}'
    
    # Criar scatter plot
    fig = px.scatter(
        df_filtered,
        x='dia_semana',
        y='hora',
        size='num_usuarios',  # Tamanho dos pontos baseado no n√∫mero de usu√°rios
        color='categoria' if categoria_selecionada == "Todas" else 'num_usuarios',  # Cor por categoria quando mostrar todas
        title=titulo,
        labels={
            'dia_semana': 'Dia da Semana',
            'hora': 'Hora do Dia',
            'num_usuarios': 'N√∫mero de Usu√°rios √önicos',
            'categoria': 'Categoria'
        },
        hover_data={
            'dia_semana': False,  # N√£o mostrar o n√∫mero do dia
            'hora': True,
            'num_usuarios': True,
            'categoria': True if categoria_selecionada == "Todas" else False
        }
    )
    
    # Personalizar o layout
    fig.update_layout(
        plot_bgcolor=THEME_COLORS['background'],
        paper_bgcolor=THEME_COLORS['background'],
        font=dict(color=THEME_COLORS['text']),
        xaxis=dict(
            ticktext=['Domingo', 'Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado'],
            tickvals=[1, 2, 3, 4, 5, 6, 7],
            gridcolor='rgba(128, 128, 128, 0.2)',
            title_font=dict(size=14)
        ),
        yaxis=dict(
            ticktext=[f'{i:02d}:00' for i in range(24)],
            tickvals=list(range(24)),
            gridcolor='rgba(128, 128, 128, 0.2)',
            title_font=dict(size=14)
        ),
        coloraxis_colorbar_title='N√∫mero de Usu√°rios' if categoria_selecionada != "Todas" else 'Categoria',
        showlegend=True,
        height=700  # Aumentar altura do gr√°fico para melhor visualiza√ß√£o
    )
    
    st.plotly_chart(fig, use_container_width=True)

    st.markdown("""
    ### üìä Insights sobre Padr√µes Temporais
    
    - **Padr√µes por Categoria**:
        - Diferentes categorias mostram padr√µes √∫nicos de consumo
        - Hor√°rios de pico variam por tipo de conte√∫do
        - Comportamentos distintos entre dias √∫teis e fins de semana
    
    - **Comportamento dos Usu√°rios**:
        - Prefer√™ncias claras por hor√°rios espec√≠ficos
        - Varia√ß√£o significativa no engajamento ao longo do dia
        - Padr√µes consistentes por categoria
    
    - **Oportunidades Identificadas**:
        1. **Timing de Publica√ß√£o**:
            - Alinhar publica√ß√µes com picos de audi√™ncia
            - Programar conte√∫do baseado em padr√µes hist√≥ricos
            - Otimizar notifica√ß√µes por categoria
    
        2. **Personaliza√ß√£o Temporal**:
            - Recomendar conte√∫do baseado no hor√°rio
            - Adaptar mix de categorias ao momento do dia
            - Considerar contexto temporal nas recomenda√ß√µes
    
        3. **Estrat√©gias de Engajamento**:
            - Identificar janelas de oportunidade por categoria
            - Desenvolver estrat√©gias espec√≠ficas por per√≠odo
            - Maximizar alcance em hor√°rios de pico
    """)

def show_advanced_patterns(data_cache):
    st.subheader("üîÑ Padr√µes Avan√ßados de Leitura")
    
    show_objective("""
    An√°lise aprofundada dos padr√µes de leitura para identificar comportamentos sazonais 
    e sequ√™ncias de consumo de conte√∫do, visando melhorar as recomenda√ß√µes.
    """)
    
    # Usar dados do cache
    seasonality = data_cache['seasonality']
    sequences = data_cache['reading_sequence']
    
    # Mapa de calor de sazonalidade
    st.subheader("üìä Padr√£o de Leitura por Dia e Hora")
    fig_heat = px.density_heatmap(
        seasonality,
        x='hora',
        y='dia_semana',
        z='total_leituras',
        title='Distribui√ß√£o de Leituras ao Longo da Semana',
        labels={
            'hora': 'Hora do Dia',
            'dia_semana': 'Dia da Semana',
            'total_leituras': 'Volume de Leituras'
        }
    )
    fig_heat.update_layout(
        plot_bgcolor=THEME_COLORS['background'],
        paper_bgcolor=THEME_COLORS['background'],
        font=dict(color=THEME_COLORS['text'])
    )
    st.plotly_chart(fig_heat, use_container_width=True)
    
    # Insights sobre sazonalidade
    st.markdown("""
    ### üìÖ Insights sobre Padr√µes Temporais
    - **Picos de Atividade**: Maior volume de leituras durante hor√°rios comerciais (10h-15h)
    - **Dias √öteis vs. Fim de Semana**: Padr√£o distinto de consumo entre dias da semana
    - **Per√≠odos de Baixa**: Menor atividade durante madrugada (0h-5h)
    - **Oportunidades**:
        - Programar envio de recomenda√ß√µes antes dos hor√°rios de pico
        - Adaptar conte√∫do recomendado conforme per√≠odo do dia
        - Estrat√©gias espec√≠ficas para aumentar engajamento em per√≠odos de baixa
    """)
    
    # Visualiza√ß√£o de sequ√™ncia de leitura
    st.subheader("üîÑ Sequ√™ncias de Leitura Mais Comuns")
    fig_seq = px.bar(
        sequences.head(10),
        x='frequency',
        y='prev_article',
        orientation='h',
        title='Top 10 Sequ√™ncias de Leitura',
        labels={
            'frequency': 'Frequ√™ncia',
            'prev_article': 'Artigo Anterior'
        }
    )
    fig_seq.update_layout(
        plot_bgcolor=THEME_COLORS['background'],
        paper_bgcolor=THEME_COLORS['background'],
        font=dict(color=THEME_COLORS['text'])
    )
    st.plotly_chart(fig_seq, use_container_width=True)
    
    # Insights sobre sequ√™ncias
    st.markdown("""
    ### üîÑ Insights sobre Sequ√™ncias de Leitura
    - **Padr√µes de Navega√ß√£o**: Identificadas sequ√™ncias frequentes de leitura entre artigos relacionados
    - **Conte√∫do √Çncora**: Alguns artigos funcionam como "hub", levando a m√∫ltiplas leituras subsequentes
    - **Comportamento do Usu√°rio**:
        - Tend√™ncia a seguir temas relacionados em sequ√™ncia
        - Forte correla√ß√£o entre artigos de economia/concursos
    
    ### üí° Recomenda√ß√µes Estrat√©gicas
    1. **Personaliza√ß√£o Temporal**:
        - Adaptar recomenda√ß√µes ao hor√°rio e dia da semana
        - Priorizar conte√∫do relevante nos hor√°rios de pico
    
    2. **Sequenciamento Inteligente**:
        - Utilizar padr√µes de sequ√™ncia para prever pr√≥ximas leituras
        - Recomendar conte√∫do baseado em caminhos de leitura comuns
    
    3. **Otimiza√ß√£o de Conte√∫do**:
        - Identificar e promover conte√∫dos "√¢ncora"
        - Criar clusters de conte√∫do baseados em padr√µes de sequ√™ncia
    """)

def show_conclusion():
    st.subheader("üîç Conclus√£o da An√°lise")
      
    st.markdown("""
    Ap√≥s uma an√°lise detalhada dos dados, chegamos √†s seguintes conclus√µes:

    - **Diversidade de Usu√°rios**: Identificamos uma base diversificada de usu√°rios com diferentes padr√µes de consumo, o que sugere a necessidade de personaliza√ß√£o.
    - **Conte√∫do Popular**: Certas categorias de conte√∫do geram mais engajamento, indicando √°reas de foco para futuras recomenda√ß√µes.
    - **Padr√µes Temporais**: Observamos padr√µes claros de intera√ß√£o ao longo do tempo, o que pode ser utilizado para otimizar o timing das recomenda√ß√µes.
    - **Desafios do Cold Start**: Estrat√©gias espec√≠ficas s√£o necess√°rias para engajar novos usu√°rios e melhorar sua experi√™ncia inicial.

    Continuaremos a monitorar e ajustar nosso sistema de recomenda√ß√£o com base nesses insights para oferecer uma experi√™ncia cada vez mais personalizada e eficaz.
    """)

def show_home():
    st.markdown("""
    Bem-vindo ao Dashboard G1, uma plataforma interativa para an√°lise explorat√≥ria de dados do nosso sistema de recomenda√ß√£o. 
    Este dashboard foi desenvolvido para fornecer insights valiosos sobre o comportamento dos usu√°rios e o desempenho do conte√∫do.

    ### Objetivo:
    - **Entender o Perfil dos Usu√°rios**: Identificar padr√µes de comportamento e segmentar usu√°rios com base em suas intera√ß√µes.
    - **Analisar o Desempenho do Conte√∫do**: Avaliar quais tipos de conte√∫do geram mais engajamento e como os usu√°rios interagem com eles.
    - **Explorar Padr√µes Temporais**: Descobrir tend√™ncias e sazonalidades nas intera√ß√µes dos usu√°rios ao longo do tempo.
    - **Abordar o Desafio do Cold Start**: Desenvolver estrat√©gias para melhorar a experi√™ncia de novos usu√°rios com base em dados limitados.

    Navegue pelas diferentes se√ß√µes para explorar os dados e descobrir insights que podem ajudar a otimizar nosso sistema de recomenda√ß√£o.
    """)

if __name__ == "__main__":
    main() 