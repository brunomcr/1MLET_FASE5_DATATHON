services:
  etl:
    build: 
      context: .
      dockerfile: Dockerfile.etl
    image: datathon-etl
    volumes:
      - ./:/app
      - ./datalake:/app/datalake
    environment:
      - SPARK_LOCAL_IP=127.0.0.1
      - PYTHONUNBUFFERED=1
      - SPARK_DRIVER_MEMORY=8g
      - SPARK_EXECUTOR_MEMORY=6g
    deploy:
      resources:
        limits:
          memory: 10g
          cpus: '4'
    tmpfs:
      - /tmp:exec,size=100G  # Aumentar de 50G para 100G
    user: "${UID}:${GID}"  # Usar mesmo usu√°rio do host

  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    image: datathon-jupyter
    ports:
      - "8888:8888"
      - "4040:4040"  # Spark UI
    volumes:
      - ./:/app
      - ./datalake:/app/datalake
    environment:
      - SPARK_LOCAL_IP=jupyter
      - JUPYTER_ENABLE_LAB=yes
      - PYTHONUNBUFFERED=1
      # Spark configurations
      - SPARK_DRIVER_MEMORY=8g
      - SPARK_EXECUTOR_MEMORY=8g
      - SPARK_EXECUTOR_CORES=4
      - SPARK_WORKER_MEMORY=8g
      - SPARK_WORKER_CORES=4
    deploy:
      resources:
        limits:
          memory: 6g
          cpus: '4'
    shm_size: '2gb'

  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    image: datathon-streamlit
    ports:
      - "8501:8501"
    volumes:
      - ./:/app
      - ./datalake:/app/datalake
    environment:
      - SPARK_LOCAL_IP=streamlit
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        limits:
          memory: 6g
          cpus: '4'